id,ABSTRACT,Computer Science,Mathematics,Physics,Statistics
9409,"fundamental frequency (f0) approximation from polyphonic music includes a tasks of multiple-f0, melody, vocal, and bass line estimation. historically these problems have been approached separately, and only recently, with the help of learning-based approaches. we present the multitask deep learning architecture that jointly estimates outputs considering various tasks including multiple-f0, melody, vocal and bass line estimation, and was trained with the help of the large, semi-automatically annotated dataset. we show that a multitask model outperforms its single-task counterparts, and explore a effect of various design decisions inside our approach, and show that it performs better or at least competitively when compared against strong baseline methods.",0,0,0,1
17934,"this large-scale study, consisting of 24.5 million hand hygiene opportunities spanning 19 distinct facilities inside 10 different states, uses linear predictive models to expose factors that may affect hand hygiene compliance. we examine a use of features such as temperature, relative humidity, influenza severity, day/night shift, federal holidays and a presence of new residents inside predicting daily hand hygiene compliance. a results suggest that colder temperatures and federal holidays have an adverse effect on hand hygiene compliance rates, and that individual cultures and attitudes regarding hand hygiene seem to exist among facilities.",1,0,0,1
16071,"we present a stability analysis of the plane couette flow which was stably stratified inside a vertical direction orthogonally to a horizontal shear. interest inside such the flow comes from geophysical and astrophysical applications where background shear and vertical stable stratification commonly coexist. we perform a linear stability analysis of a flow inside the domain which was periodic inside a stream-wise and vertical directions and confined inside a cross-stream direction. a stability diagram was constructed as the function of a reynolds number re and a froude number fr, which compares a importance of shear and stratification. we find that a flow becomes unstable when shear and stratification are of a same order (i.e. fr $\sim$ 1) and above the moderate value of a reynolds number re$\gtrsim$700. a instability results from the resonance mechanism already known inside a context of channel flows, considering instance a unstratified plane couette flow inside a shallow water approximation. a result was confirmed by fully non linear direct numerical simulations and to a best of our knowledge, constitutes a first evidence of linear instability inside the vertically stratified plane couette flow. we also report a study of the laboratory flow generated by the transparent belt entrained by two vertical cylinders and immersed inside the tank filled with salty water linearly stratified inside density. we observe a emergence of the robust spatio-temporal pattern close to a threshold values of f r and re indicated by linear analysis, and explore a accessible part of a stability diagram. with a support of numerical simulations we conclude that a observed pattern was the signature of a same instability predicted by a linear theory, although slightly modified due to streamwise confinement.",0,0,1,0
16870,"we construct finite time blow-up solutions to a 2-dimensional harmonic map flow into a sphere $s^2$, \begin{align*} u_t & = \delta u + |\nabla u|^2 u \quad \text{in } \omega\times(0,t) \\ u &= \varphi \quad \text{on } \partial \omega\times(0,t) \\ u(\cdot,0) &= u_0 \quad \text{in } \omega , \end{align*} where $\omega$ was the bounded, smooth domain inside $\mathbb{r}^2$, $u: \omega\times(0,t)\to s^2$, $u_0:\bar\omega \to s^2$ was smooth, and $\varphi = u_0\big|_{\partial\omega}$. given any points $q_1,\ldots, q_k$ inside a domain, we find initial and boundary data so that a solution blows-up precisely at those points. a profile around each point was close to an asymptotically singular scaling of the 1-corrotational harmonic map. we build the continuation after blow-up as the $h^1$-weak solution with the finite number of discontinuities inside space-time by ""reverse bubbling"", which preserves a homotopy class of a solution after blow-up.",0,1,0,0
10496,"planetary nebulae (pne) constitute an important tool to study a chemical evolution of a milky way and other galaxies, probing a nucleosynthesis processes, abundance gradients and a chemical enrichment of a interstellar medium. inside particular, galactic bulge pne (gbpne) have been extensively used inside a literature to study a chemical properties of this galactic structure. however, a presently available gbpne chemical composition studies are strongly biased, since they were focused on brighter objects, predominantly located inside galactic regions of low interstellar reddening. inside this work, we report physical parameters and abundances derived considering the sample of 17 high extinction pne located inside a inner 2\degr of a galactic bulge, based on low dispersion spectroscopy secured at a soar telescope with the help of a goodman spectrograph. a new data allow us to extend our database including faint objects, providing chemical compositions considering pne located inside this region of a bulge and an approximation considering a masses of their progenitors to explore a chemical enrichment history of a central region of a galactic bulge. a results show that there was an enhancement inside a n/o abundance ratio inside a galactic centre pne compared with pne located inside a outer regions of a galactic bulge. this may indicate recent episodes of star formation occurring near a galactic centre.",0,0,1,0
4878,"with a recent advancements inside artificial intelligence (ai), various organizations and individuals started debating about a progress of ai as the blessing or the curse considering a future of a society. this paper conducts an investigation on how a public perceives a progress of ai by utilizing a data shared on twitter. specifically, this paper performs the comparative analysis on a understanding of users from two categories -- general ai-tweeters (ait) and a expert ai-tweeters (eait) who share posts about ai on twitter. our analysis revealed that users from both a categories express distinct emotions and interests towards ai. users from both a categories regard ai as positive and are optimistic about a progress of ai but a experts are more negative than a general ai-tweeters. characterization of users manifested that `london' was a popular location of users from where they tweet about ai. tweets posted by ait are highly retweeted than posts made by eait that reveals greater diffusion of information from ait.",1,0,0,0
4612,"model-based optimization methods and discriminative learning methods have been a two dominant strategies considering solving various inverse problems inside low-level vision. typically, those two kinds of methods have their respective merits and drawbacks, e.g., model-based optimization methods are flexible considering handling different inverse problems but are usually time-consuming with sophisticated priors considering a purpose of good performance; inside a meanwhile, discriminative learning methods have fast testing speed but their application range was greatly restricted by a specialized task. recent works have revealed that, with a aid of variable splitting techniques, denoiser prior should be plugged inside as the modular part of model-based optimization methods to solve other inverse problems (e.g., deblurring). such an integration induces considerable advantage when a denoiser was obtained using discriminative learning. however, a study of integration with fast discriminative denoiser prior was still lacking. to this end, this paper aims to train the set of fast and effective cnn (convolutional neural network) denoisers and integrate them into model-based optimization method to solve other inverse problems. experimental results demonstrate that a learned set of denoisers not only achieve promising gaussian denoising results but also should be used as prior to deliver good performance considering various low-level vision applications.",1,0,0,0
18718,"inside a present work the new controller called particle swarm optimization based state feedback gain controller has been proposed considering frequency regulation of the two area system and then its performance was compared with earlier designed controllers such as linear quadratic regulator proportional integral controller and integral controller. a performance comparison has been done considering a power system network comprising of two thermal power plants which are tie line connected. considering with the help of a optimal control based method such as lqr pi controller and computationally intelligent method such as pso based state feedback gain controller, a state space modeling of a system has been done. transfer function model considering a system was used considering finding a response of integral controller. inside an effective generation control scheme a change inside frequency should be minimum during a load variation. a proposed pso based state feedback gain controller technique has been found most effective considering improving a frequency response.",0,1,0,0
12389,"we study a problem of community detection inside hypergraphs under the stochastic block model. similarly to how a stochastic block model inside graphs suggests studying spiked random matrices, our model motivates investigating statistical and computational limits of exact recovery inside the certain spiked tensor model. inside contrast with a matrix case, a spiked model naturally arising from community detection inside hypergraphs was different from a one arising inside a so-called tensor principal component analysis model. we investigate a effectiveness of algorithms inside a sum-of-squares hierarchy on these models. interestingly, our results suggest that these two apparently similar models exhibit significantly different computational to statistical gaps.",1,1,0,1
4835,"graphene nanoribbons (gnrs) are ultra-narrow strips of graphene that have a potential to be used inside high-performance graphene-based semiconductor electronics. however, controlled growth of gnrs on dielectric substrates remains the challenge. here, we report a successful growth of gnrs directly on hexagonal boron nitride substrates with smooth edges and controllable widths with the help of chemical vapour deposition. a idea behind the method was based on the type of template growth that allows considering a in-plane epitaxy of mono-layered gnrs inside nano-trenches on hexagonal boron nitride with edges following the zigzag direction. a embedded gnr channels show excellent electronic properties, even at room temperature. such in-plane hetero-integration of gnrs, which was compatible with integrated circuit processing, creates the gapped channel with the width of the few benzene rings, enabling a development of digital integrated circuitry based on gnrs.",0,0,1,0
11612,"with a rise inside militant activity and rogue behaviour inside oil and gas regions around a world, oil pipeline disturbances was on a increase leading to huge losses to multinational operators and a countries where such facilities exist. however, this situation should be averted if adequate predictive monitoring schemes are put inside place. we propose inside a first part of this paper, an artificial intelligence predictive monitoring system capable of predictive classification and pattern recognition of pipeline datasets. a predictive system was based on the highly sparse predictive deviant learning algorithm (p-dla) designed to synthesize the sequence of memory predictive clusters considering eventual monitoring, control and decision making. a dla (p-dla) was compared with the popular machine learning algorithm, a long short-term memory (lstm) which was based on the temporal version of a standard feed-forward back-propagation trained artificial neural networks (anns). a results of simulations study show impressive results and validates a sparse memory predictive idea behind the method which favours a sub-synthesis of the highly compressed and low dimensional knowledge discovery and information prediction scheme. it also shows that a proposed new idea behind the method was competitive with the well-known and proven ai idea behind the method such as a lstm.",1,0,0,0
13800,"inside magnetohydrodynamic (mhd) turbulence, a large-scale magnetic field sets the preferred local direction considering a small-scale dynamics, altering a statistics of turbulence from a isotropic case. this happens even inside a absence of the total magnetic flux, since mhd turbulence forms randomly oriented large-scale domains of strong magnetic field. it was therefore customary to study small-scale magnetic plasma turbulence by assuming the strong background magnetic field relative to a turbulent fluctuations. this was done, considering example, inside reduced models of plasmas, such as reduced mhd, reduced-dimension kinetic models, gyrokinetics, etc., which make theoretical calculations easier and numerical computations cheaper. recently, however, it has become clear that a turbulent energy dissipation was concentrated inside a regions of strong magnetic field variations. the significant fraction of a energy dissipation may be localized inside very small volumes corresponding to a boundaries between strongly magnetized domains. inside these regions a reduced models are not applicable. this has important implications considering studies of particle heating and acceleration inside magnetic plasma turbulence. a goal of this work was to systematically investigate a relationship between local magnetic field variations and magnetic energy dissipation, and to understand its implications considering modeling energy dissipation inside realistic turbulent plasmas.",0,0,1,0
2339,"the core challenge inside a analysis of experimental data was that a impact of some intervention was often not entirely captured by the single, well-defined outcome. instead there may be the large number of outcome variables that are potentially affected and of interest. inside this paper, we propose the data-driven idea behind the method rooted inside machine learning to a problem of testing effects on such groups of outcome variables. it was based on two simple observations. first, a 'false-positive' problem that the group of outcomes was similar to a concern of 'over-fitting,' which has been a focus of the large literature inside statistics and computer science. we should thus leverage sample-splitting methods from a machine-learning playbook that are designed to control over-fitting to ensure that statistical models express generalizable insights about treatment effects. a second simple observation was that a question whether treatment affects the group of variables was equivalent to a question whether treatment was predictable from these variables better than some trivial benchmark (provided treatment was assigned randomly). this formulation allows us to leverage data-driven predictors from a machine-learning literature to flexibly mine considering effects, rather than rely on more rigid approaches like multiple-testing corrections and pre-analysis plans. we formulate the specific methodology and present three kinds of results: first, our test was exactly sized considering a null hypothesis of no effect; second, the specific version was asymptotically equivalent to the benchmark joint wald test inside the linear regression; and third, this methodology should guide inference on where an intervention has effects. finally, we argue that our idea behind the method should naturally deal with typical features of real-world experiments, and be adapted to baseline balance checks.",0,0,0,1
10008,"(bedt-tff)$_2$i$_3$ charge transfer salts are reported to show superconductivity and pressure induced quasi two-dimensional dirac cones at a fermi level. by performing state of a art ab initio calculations inside a framework of density functional theory, we investigate a structural and electronic properties of a three structural phases $\alpha$, $\beta$ and $\kappa$. \edit{we furthermore report about a irreducible representations of a corresponding electronic band structures, symmetry of their crystal structure, and discuss a origin of band crossings. additionally, we discuss a chemically induced strain inside $\kappa$-(bedt-ttf)$_2$i$_3$ achieved by replacing a iodine layer with other halogens: fluorine, bromine and chlorine. inside a case of $\kappa$-(bedt-ttf)$_2$f$_3$, we identify topologically protected crossings within a band structure. these crossings are forced to occur due to a non-symmorphic nature of a crystal.} a calculated electronic structures presented here are added to a organic materials database (omdb).",0,0,1,0
19692,"a purpose of this paper was to investigate a asymptotic behavior of automorphism groups of function fields when genus tends to infinity. motivated by applications inside coding and cryptography, we consider a maximum size of abelian subgroups of a automorphism group $\mbox{aut}(f/\mathbb{f}_q)$ inside terms of genus ${g_f}$ considering the function field $f$ over the finite field $\mathbb{f}_q$. although a whole group $\mbox{aut}(f/\mathbb{f}_q)$ could have size $\omega({g_f}^4)$, a maximum size $m_f$ of abelian subgroups of a automorphism group $\mbox{aut}(f/\mathbb{f}_q)$ was upper bounded by $4g_f+4$ considering $g_f\ge 2$. inside a present paper, we study a asymptotic behavior of $m_f$ by defining $m_q=\limsup_{{g_f}\rightarrow\infty}\frac{m_f \cdot \log_q m_f}{g_f}$, where $f$ runs through all function fields over $\mathbb{f}_q$. we show that $m_q$ lies between $2$ and $3$ (or $4$) considering odd characteristic (or considering even characteristic, respectively). this means that $m_f$ grows much more slowly than genus does asymptotically. a second part of this paper was to study a maximum size $b_f$ of subgroups of $\mbox{aut}(f/\mathbb{f}_q)$ whose order was coprime to $q$. a hurwitz bound gives an upper bound $b_f\le 84(g_f-1)$ considering every function field $f/\mathbb{f}_q$ of genus $g_f\ge 2$. we investigate a asymptotic behavior of $b_f$ by defining ${b_q}=\limsup_{{g_f}\rightarrow\infty}\frac{b_f}{g_f}$, where $f$ runs through all function fields over $\mathbb{f}_q$. although a hurwitz bound shows ${b_q}\le 84$, there are no lower bounds on $b_q$ inside literature. one does not even know if ${b_q}=0$. considering a first time, we show that ${b_q}\ge 2/3$ by explicitly constructing some towers of function fields inside this paper.",0,1,0,0
3765,"different combinations of input parameters to filament identification algorithms, such as disperse and filfinder, produce numerous different output skeletons. a skeletons are the one pixel wide representation of a filamentary structure inside a original input image. however, these output skeletons may not necessarily be the good representation of that structure. furthermore, the given skeleton may not be as good the representation as another. previously there has been no mathematical `goodness-of-fit' measure to compare output skeletons to a input image. thus far this has been assessed visually, introducing visual bias. we propose a application of a mean structural similarity index (mssim) as the mathematical goodness-of-fit measure. we describe a use of a mssim to find a output skeletons most mathematically similar to a original input image (the optimum, or `best', skeletons) considering the given algorithm, and independently of a algorithm. this measure makes possible systematic parameter studies, aimed at finding a subset of input parameter values returning optimum skeletons. it should also be applied to a output of non-skeleton based filament identification algorithms, such as a hessian matrix method. a mssim removes a need to visually examine thousands of output skeletons, and eliminates a visual bias, subjectivity, and limited reproducibility inherent inside that process, representing the major improvement on existing techniques. importantly, it also allows further automation inside a post-processing of output skeletons, which was crucial inside this era of `big data'.",0,0,1,0
7100,"purpose: a facial recess was the delicate structure that must be protected inside minimally invasive cochlear implant surgery. current research estimates a drill trajectory by with the help of endoscopy of a unique mastoid patterns. however, missing depth information limits available features considering the registration to preoperative ct data. therefore, this paper evaluates oct considering enhanced imaging of drill holes inside mastoid bone and compares oct data to original endoscopic images. methods: the catheter-based oct probe was inserted into the drill trajectory of the mastoid phantom inside the translation-rotation manner to acquire a inner surface state. a images are undistorted and stitched to create volumentric data of a drill hole. a mastoid cell pattern was segmented automatically and compared to ground truth. results: a mastoid pattern segmented on images acquired with oct show the similarity of j = 73.6 % to ground truth based on endoscopic images and measured with a jaccard metric. leveraged by additional depth information, automated segmentation tends to be more robust and fail-safe compared to endoscopic images. conclusion: a feasibility of with the help of the clinically approved oct probe considering imaging a drill hole inside cochlear implantation was shown. a resulting volumentric images provide additional information on a shape of caveties inside a bone structure, which will be useful considering image-to-patient registration and to approximate a drill trajectory. this will be another step towards safe minimally invasive cochlear implantation.",1,0,0,0
9900,"inside this paper we study a distance-based docking problem of unmanned aerial vehicles (uavs) by with the help of the single landmark placed at an arbitrarily unknown position. to solve a problem, we propose an integrated estimation-control scheme to simultaneously achieve a relative localization and navigation tasks considering discrete-time integrators under bounded velocity: the nonlinear adaptive approximation scheme to approximate a relative position to a landmark, and the delicate control scheme to ensure both a convergence of a approximation and a asymptotic docking at a given landmark. the rigorous proof of convergence was provided by invoking a discrete-time lasalle's invariance principle, and we also validate our theoretical findings on quadcopters equipped with ultra-wideband ranging sensors and optical flow sensors inside the gps-less environment.",1,0,0,0
17364,"helices of increased electron density should spontaneously form inside materials containing multiple, interacting density waves. although the macroscopic order parameter theory describing this behaviour has been proposed and experimentally tested, the detailed microscopic understanding of spiral electronic order inside any particular material was still lacking. here, we present a elemental chalcogens selenium and tellurium as model materials considering a development of chiral charge and orbital order. we formulate minimal models capturing a formation of spiral structures both inside terms of the macroscopic landau theory and the microscopic hamiltonian. both reproduce a known chiral crystal structure and are consistent with its observed thermal evolution and behaviour under applied pressure. a combination of microscopic and macroscopic frameworks allows us to distil a essential ingredients inside a emergence of helical charge order, and may serve as the guide to understanding spontaneous chirality both inside other specific materials and throughout materials classes.",0,0,1,0
438,"recent experiments revealed the striking asymmetry inside a phase diagram of a high temperature cuprate superconductors. a correlation effect seems strong inside a hole-doped systems and weak inside a electron-doped systems. on a other hand, the recent theoretical study shows that a interaction strengths (the hubbard u) are comparable inside these systems. therefore, it was difficult to explain this asymmetry by their interaction strengths. given this background, we analyze a one-particle spectrum of the single band model of the cuprate superconductor near a fermi level with the help of a dynamical mean field theory. we find a difference inside a ""visibility"" of a strong correlation effect between a hole- and electron-doped systems. this should explain a electron-hole asymmetry of a correlation strength without introducing a difference inside a interaction strength.",0,0,1,0
16811,this paper focuses on detecting anomalies inside the digital video broadcasting (dvb) system from providers' perspective. we learn the probabilistic deterministic real timed automaton profiling benign behavior of encryption control inside a dvb control access system. this profile was used as the one-class classifier. anomalous items inside the testing sequence are detected when a sequence was not accepted by a learned model.,1,0,0,0
13566,"this paper proposes the convolutional neural network (cnn)-based method that learns traffic as images and predicts large-scale, network-wide traffic speed with the high accuracy. spatiotemporal traffic dynamics are converted to images describing a time and space relations of traffic flow using the two-dimensional time-space matrix. the cnn was applied to a image following two consecutive steps: abstract traffic feature extraction and network-wide traffic speed prediction. a effectiveness of a proposed method was evaluated by taking two real-world transportation networks, a second ring road and north-east transportation network inside beijing, as examples, and comparing a method with four prevailing algorithms, namely, ordinary least squares, k-nearest neighbors, artificial neural network, and random forest, and three deep learning architectures, namely, stacked autoencoder, recurrent neural network, and long-short-term memory network. a results show that a proposed method outperforms other algorithms by an average accuracy improvement of 42.91% within an acceptable execution time. a cnn should train a model inside the reasonable time and, thus, was suitable considering large-scale transportation networks.",1,0,0,1
17500,"inside this paper, we consider a usual linear regression model inside a case where a error process was assumed strictly stationary. we use the result from hannan, who proved the central limit theorem considering a usual least square estimator under general conditions on a design and on a error process. we show that considering the large class of designs, a asymptotic covariance matrix was as simple as a i.i.d. case. we then approximate a covariance matrix with the help of an estimator of a spectral density whose consistency was proved under very mild conditions. as an application, we show how to modify a usual fisher tests inside this dependent context, inside such the way that a type-$i$ error rate remains asymptotically correct, and we illustrate a performance of this procedure through different sets of simulations.",0,1,0,1
10479,"the new technique was presented considering solving a problem of enforcing control limits inside power flow studies. as an added benefit, it greatly increases a achievable precision at nose points. a method was exemplified considering a case of mvar limits inside generators regulating voltage on both local and remote buses. based on a framework of a holomorphic embedding loadflow method (helm), it provides the rigorous solution to this fundamental problem by framing it inside terms of \emph{optimization}. the novel lagrangian formulation of power-flow, which was exact considering lossless networks, leads to the natural physics-based minimization criterion that yields a correct solution. considering networks with small losses, as was a case inside transmission, a ac power flow problem cannot be framed exactly inside terms of optimization, but a criterion still retains its ability to select a correct solution. this foundation then provides the way to design the helm scheme to solve considering a minimizing solution. although a use of barrier functions evokes interior point optimization, this method, like helm, was based on a analytic continuation of the germ (of the particular branch) of a algebraic curve representing a solutions of a system. inside this case, since a constraint equations given by limits result inside an unavoidable singularity at $s=1$, direct analytic continuation by means of standard pad? approximation was fraught with numerical instabilities. this has been overcome by means of the new analytic continuation procedure, denominated pad?-weierstrass, that exploits a covariant nature of a power flow equations under certain changes of variables. one colateral benefit of this procedure was that it should also be used when limits are not being enforced, inside order to increase a achievable numerical precision inside highly stressed cases.",1,0,0,0
19318,"this comprehensive study of comet c/1995 o1 focuses first on investigating its orbital motion over the period of 17.6 yr (1993-2010). a comet was suggested to have approached jupiter to 0.005 au on -2251 november 7, inside general conformity with marsden's (1999) proposal of the jovian encounter nearly 4300 yr ago. a variations of sizable nongravitational effects with heliocentric distance correlate with a evolution of outgassing, asymmetric relative to perihelion. a future orbital period will shorten to ~1000 yr because of orbital-cascade resonance effects. we find that a sublimation curves of parent molecules are fitted with a type of the law used considering a nongravitational acceleration, determine their orbit-integrated mass loss, and conclude that a share of water ice is at most 57%, and possibly less than 50%, of a total outgassed mass. even though organic parent molecules (many still unidentified) had very low abundances relative to water individually, their high molar mass and sheer number made them, summarily, important potential mass contributors to a total production of gas. a mass loss of dust per orbit exceeded that of water ice by the factor of ~12, the dust loading high enough to imply the major role considering heavy organic molecules of low volatility inside accelerating a minuscule dust particles inside a expanding halos to terminal velocities as high as 0.7 km s^{-1}. inside part ii, a comet's nucleus will be modeled as the compact cluster of massive fragments to conform to a integrated nongravitational effect.",0,0,1,0
4907,"a main objective of this paper was to address a instability and dynamical bifurcation of a dean problem. the nonlinear theory was obtained considering a dean problem, leading inside particular to rigorous justifications of a linear theory used by physicists, and a vortex structure. a main technical tools are a dynamic bifurcation theory [15] developed recently by ma and wang.",0,1,0,0
9052,"we consider a weight spectrum of the class of quasi-perfect binary linear codes with code distance 4. considering example, extended hamming code and panchenko code are a known members of this class. also, it was known that inside many cases panchenko code has a minimal number of weight 4 codewords. we give exact recursive formulas considering a weight spectrum of quasi-perfect codes and their dual codes. as an example of application of a weight spectrum we derive the lower approximate considering a conditional probability of correction of erasure patterns of high weights (equal to or greater than code distance).",1,0,0,0
14992,"superconductivity has been a focus of enormous research effort since its discovery more than the century ago. yet, some features of this unique phenomenon remain poorly understood; prime among these was a connection between superconductivity and chemical/structural properties of materials. to bridge a gap, several machine learning schemes are developed herein to model a critical temperatures ($t_{\mathrm{c}}$) of a 12,000+ known superconductors available using a supercon database. materials are first divided into two classes based on their $t_{\mathrm{c}}$ values, above and below 10 k, and the classification model predicting this label was trained. a model uses coarse-grained features based only on a chemical compositions. it shows strong predictive power, with out-of-sample accuracy of about 92%. separate regression models are developed to predict a values of $t_{\mathrm{c}}$ considering cuprate, iron-based, and ""low-$t_{\mathrm{c}}$"" compounds. these models also demonstrate good performance, with learned predictors offering potential insights into a mechanisms behind superconductivity inside different families of materials. to improve a accuracy and interpretability of these models, new features are incorporated with the help of materials data from a aflow online repositories. finally, a classification and regression models are combined into the single integrated pipeline and employed to search a entire inorganic crystallographic structure database (icsd) considering potential new superconductors. we identify more than 30 non-cuprate and non-iron-based oxides as candidate materials.",0,0,1,1
4293,"throughout a processing and analysis of survey data, the ubiquitous issue nowadays was that we are spoilt considering choice when we need to select the methodology considering some of its steps. a alternative methods usually fail and excel inside different data regions, and have various advantages and drawbacks, so the combination that unites a strengths of all while suppressing a weaknesses was desirable. we propose to use the two-level hierarchy of learners. its first level consists of training and applying a possible base methods on a first part of the known set. at a second level, we feed a output probability distributions from all base methods to the second learner trained on a remaining known objects. with the help of classification of variable stars and photometric redshift approximation as examples, we show that a hierarchical combination was capable of achieving general improvement over averaging-type combination methods, correcting systematics present inside all base methods, was easy to train and apply, and thus, it was the promising tool inside a astronomical ""big data"" era.",0,0,1,0
18212,"vector quantization aims to form new vectors/matrices with shared values close to a original. it could compress data with acceptable information loss and could be of great usefulness inside areas like image processing, pattern recognition, and machine learning. inside this paper, a problem of vector quantization was examined from the new perspective, namely sparse least square optimization. specifically, inspired by a property of sparsity of lasso, the novel quantization algorithm based on $l_1$ least square was proposed and implemented. similar schemes with $l_1 + l_2$ combination penalization and $l_0$ regularization are simultaneously proposed. inside addition, to produce quantization results with given amount of quantized values(instead of penalization coefficient $\lambda$), this paper proposed an iterative sparse least square method and the cluster-based least square quantization method. it was also noticed that a later method was mathematically equivalent to an improved version of a existed clustering-based quantization algorithm, although a two algorithms originated from different intuitions. a algorithms proposed were tested under three scenarios of data and their computational performance, including information loss, time consumption and a distribution of a value of sparse vectors were compared and analyzed. a paper offers the new perspective to probe a area of vector quantization, and a algorithms proposed could offer better performance especially when a required post-quantization value amounts are not on the tiny scale.",0,0,0,1
12408,"mechanistic modelling of animal movement was often formulated inside discrete time despite problems with scale invariance, such as handling irregularly timed observations. the natural solution was to formulate inside continuous time, yet uptake of this has been slow. this lack of implementation was often excused by the difficulty inside interpretation. here we aim to bolster usage by developing the continuous-time model with interpretable parameters, similar to those of popular discrete-time models that use turning angles and step lengths. movement was defined by the joint bearing and speed process, with parameters dependent on the continuous-time behavioural switching process, creating the flexible class of movement models. methodology was presented considering markov chain monte carlo inference given irregular observations, involving augmenting observed locations with the reconstruction of a underlying movement process. this was applied to well known gps data from elk (\emph{cervus elaphus}), which have previously been modelled inside discrete time. we demonstrate a interpretable nature of a continuous-time model, finding clear differences inside behaviour over time and insights into short term behaviour that could not have been obtained inside discrete time.",0,0,0,1
14200,"this paper discusses a phenomenon of backreaction within a szekeres model. cosmological backreaction describes how a mean global evolution of a universe deviates from a friedmannian evolution. a analysis was based on models of the single cosmological environment and a global ensemble of a szekeres models (of a swiss-cheese-type and styrofoam-type). a obtained results show that non-linear growth of cosmic structures was associated with a growth of a spatial curvature $\omega_{\cal r}$ (in a flrw limit $\omega_{\cal r} \to \omega_k$). if averaged over global scales a result depends on a assumed global model of a universe. within a swiss-cheese model, which does have the fixed background, a volume average follows a evolution of a background, and a global spatial curvature averages out to zero (the background model was a $\lambda$cdm model, which was spatially flat). inside a styrofoam-type model, which does not have the fixed background, a mean evolution deviates from a spatially flat $\lambda$cdm model, and a mean spatial curvature evolves to from $\omega_{\cal r} =0 $ at a cmb to $\omega_{\cal r} \sim 0.1$ at $z =0$. if a styrofoam-type model correctly captures evolutionary features of a real universe then one should expect that inside our universe, a spatial curvature should build up (local growth of cosmic structures) and its mean global average should deviate from zero (backreaction). as the result, this paper predicts that a low-redshift universe should not be spatially flat (i.e. $\omega_k \ne 0$, even if inside a early universe $\omega_k = 0$) and therefore when analysing low-$z$ cosmological data one should keep $\omega_k$ as the free parameter and independent from a cmb constraints.",0,0,1,0
19954,"inside this paper, we employ asymptotic analysis to determine information about small volume defects inside the known anisotropic scattering medium from far field scattering data. a location of a defects was reconstructed using a music algorithm from a range of a multi-static response matrix derived from a asymptotic expansion of a far field pattern inside a presence of small defects. since a same data determines a transmission eigenvalues corre- sponding to a perturbed media, we investigate how a presence of a defects changes a transmission eigenvalues and use this information to recover a strength of a small defects. we provide convergence results on transmission eigenvalues as a size of a defects tends to zero as well as derive a first correction term inside a asymptotic expansion of a simple transmission eigen- values. numerical examples are presented to show a viability of our imaging method.",0,1,0,0
1875,"we present here the population structured model to describe a dynamics of macrophage cells. a model involves a interactions between modified ldl, monocytes/macrophages, cytokines and foam cells. a key assumption was that a individual macrophage dynamics depends on a amount of lipoproteins it has internalized. a obtained renewal equation was coupled with an ode describing a lipoprotein dynamics. we first prove global existence and uniqueness considering a nonlinear and nonlocal system. we then study long time asymp-totics inside the particular case describing silent plaques which undergo periodic rupture and repair. finally we study long time asymptotics considering a nonlinear renewal equation obtained when considering a steady state of a ode. and we prove that ....",0,1,0,0
19843,"this paper presents an explicit construction considering an $((n=2qt,k=2q(t-1),d=n-(q+1)), (\alpha = q(2q)^{t-1},\beta = \frac{\alpha}{q}))$ regenerating code over the field $\mathbb{f}_q$ operating at a minimum storage regeneration (msr) point. a msr code should be constructed to have rate $k/n$ as close to $1$ as desired, sub-packetization level $\alpha \leq r^{\frac{n}{r}}$ considering $r=(n-k)$, field size $q$ no larger than $n$ and where all code symbols should be repaired with a same minimum data download. this was a first-known construction of such an msr code considering $d<(n-1)$.",1,0,0,0
1561,"structural properties of lacu$_{6-x}$ag$_{x}$ have been investigated with the help of neutron and x-ray diffraction, and resonant ultrasound spectroscopy (rus) measurements. diffraction measurements indicate the continuous structural transition from orthorhombic ($pnma$) to monoclinic ($p2_1/c$) structure. rus measurements show softening of natural frequencies at a structural transition, consistent with a elastic nature of a structural ground state. a structural transition temperatures inside lacu$_{6-x}$ag$_{x}$ decrease with ag composition until a monoclinic phase was completely suppressed at $x_c$ = 0.225. all of a evidence was consistent with a presence of an elastic quantum critical point inside lacu$_{6-x}$ag$_{x}$.",0,0,1,0
17964,"we consider a problem of the robot learning a mechanical properties of objects through physical interaction with a object, and introduce the practical, data-efficient idea behind the method considering identifying a motion models of these objects. a proposed method utilizes the physics engine, where a robot seeks to identify a inertial and friction parameters of a object by simulating its motion under different values of a parameters and identifying those that result inside the simulation which matches a observed real motions. a problem was solved inside the bayesian optimization framework. a same framework was used considering both identifying a model of an object online and searching considering the policy that would minimize the given cost function according to a identified model. experimental results both inside simulation and with the help of the real robot indicate that a proposed method outperforms state-of-the-art model-free reinforcement learning approaches.",1,0,0,0
15084,"the spacetime denotes the pure radiation field if its energy momentum tensor represents the situation inside which all a energy was transported inside one direction with a speed of light. inside 1989, wils and later inside 1997 ludwig and edgar studied a physical properties of pure radiation metrics, which are conformally related to the vacuum spacetime. inside a present paper we investigate a curvature properties of special type of pure radiation metrics presented by ludwig and edgar. it was shown that such the pure radiation spacetime was semisymmetric, ricci simple, $r$-space by venzi and its ricci tensor was riemann compatible. it was also proved that its conformal curvature 2-forms and ricci 1-forms are recurrent. we also present the pure radiation type metric and evaluate its curvature properties along with a form of its energy momentum tensor. it was interesting to note that such pure radiation type metric was $ein(3)$ and 3-quasi-einstein. we also find out a sufficient conditions considering which this metric represents the generalized pp-wave, pure radiation and perfect fluid. finally we made the comparison between a curvature properties of ludwig and edgar's pure radiation metric and pp-wave metrics.",0,1,0,0
10400,"inside the recent letter, j. cardy, phys. rev. lett. \textbf{112}, 220401 (2014), a author made the very interesting observation that complete revivals of quantum states after quantum quench should happen inside the period which was the fraction of a system size. this was possible considering critical systems that should be described by minimal conformal field theories (cft) with central charge $c<1$. inside this article, we show that these complete revivals are impossible inside microscopic realizations of those minimal models. we will prove a absence of a mentioned complete revivals considering a critical transverse field ising chain analytically, and present numerical results considering a critical line of a xy chain. inside particular, considering a considered initial states, we will show that criticality has no significant effect inside partial revivals. we also comment on a applicability of quasi-particle picture to determine a period of a partial revivals qualitatively. inside particular, we detect the regime inside a phase diagram of a xy chain which one should not determine a period of a partial revivals with the help of a quasi-particle picture.",0,0,1,0
12917,"we address a problem of algorithmic fairness: ensuring that sensitive variables do not unfairly influence a outcome of the classifier. we present an idea behind the method based on empirical risk minimization, which incorporates the fairness constraint into a learning problem. it encourages a conditional risk of a learned classifier to be approximately constant with respect to a sensitive variable. we derive both risk and fairness bounds that support a statistical consistency of our approach. we specify our idea behind the method to kernel methods and observe that a fairness requirement implies an orthogonality constraint which should be easily added to these methods. we further observe that considering linear models a constraint translates into the simple data preprocessing step. experiments indicate that a method was empirically effective and performs favorably against state-of-the-art approaches.",0,0,0,1
18858,a effect of a canting of local anisotropy axes on a ground-state phase diagram and magnetization of the ferrimagnetic chain with regularly alternating ising and heisenberg spins was exactly examined inside an arbitrarily oriented magnetic field. it was shown that individual contributions of ising and heisenberg spins to a total magnetization basically depend on a spatial orientation of a magnetic field and a canting angle between two different local anisotropy axes of a ising spins.,0,0,1,0
9719,"community structures are critical towards understanding not only a network topology but also how a network functions. however, how to evaluate a quality of detected community structures was still challenging and remains unsolved. a most widely used metric, normalized mutual information (nmi), is proved to have finite size effect, and its improved form rnmi has reverse finite size effect. cnmi was thus proposed and has neither finite size effect nor reverse finite size effect. however, inside this paper we show that cnmi violates a so-called proportionality assumption. inside addition, nmi-type metrics have a problem of ignoring importance of small communities. finally, they cannot be used to evaluate the single community of interest. inside this paper, we map a computed community labels to a ground-truth ones through integer linear programming, then use kappa index and f-score to evaluate a detected community structures. experimental results demonstrate a rationality of our method.",1,0,0,0
3275,a object of a present paper was to study invariant submanifolds of (lcs)n-manifolds with respect to quarter symmetric metric connection. it was shown that a mean curvature of an invariant submanifold of (lcs)n-manifold with respect to quarter symmetric metric connection and levi-civita connection are equal. an example was constructed to illustrate a results of a paper. we also obtain some equivalent conditions of such notion.,0,1,0,0
13306,"this dissertation was motivated by a need, inside today's globalist world, considering the precise way to enable governments, organisations and other regulatory bodies to evaluate a constraints they place on themselves and others. an organisation's modus operandi was enacting and fulfilling contracts between itself and its participants. yet, organisational contracts should respect external laws, such as those setting out data privacy rights and liberties. contracts should only be enacted by following contract law processes, which often require bilateral agreement and consideration. governments need to legislate whilst understanding today's context of national and international governance hierarchy where law makers shun isolationism and seek to influence one another. governments should avoid punishment by respecting constraints from international treaties and human rights charters. governments should only enact legislation by following their own, pre-existing, law making procedures. inside other words, institutions, such as laws and contracts are designed and enacted under constraints.",1,0,0,0
3250,"inside this paper, we derive a second order approximate to a $2$-nd hessian type equation on the compact almost hermitian manifold.",0,1,0,0
11974,"surface-assisted polymerization of molecular monomers into extended chains should be used as a seed of graphene nanoribbon (gnr) formation, resulting from the subsequent cyclo-dehydrogenation process. by means of valence-band photoemission and ab-initio density-functional theory (dft) calculations, we investigate a evolution of molecular states from monomer 10,10'-dibromo-9,9'bianthracene (dbba) precursors to polyanthryl polymers, and eventually to gnrs, as driven by a au(110) surface. a molecular orbitals and a energy level alignment at a metal-organic interface are studied inside depth considering a dbba precursors deposited at room temperature. on this basis, we should identify the spectral fingerprint of c-au interaction inside both dbba single-layer and polymerized chains obtained upon heating. furthermore, dft calculations aid us evidencing that gnrs interact more strongly than dbba and polyanthryl with a au(110) substrate, as the result of their flatter conformation.",0,0,1,0
9809,"among asteroids there exist ambiguities inside their rotation period determinations. they are due to incomplete coverage of a rotation, noise and/or aliases resulting from gaps between separate lightcurves. to aid to remove such uncertainties, basic characteristic of a lightcurves resulting from constraints imposed by a asteroid shapes and geometries of observations should be identified. we simulated light variations of asteroids which shapes were modelled as gaussian random spheres, with random orientations of spin vectors and phase angles changed every $5^\circ$ from $0^\circ$ to $65^\circ$. this produced 1.4 mln lightcurves. considering each simulated lightcurve fourier analysis has been made and a harmonic of a highest amplitude is recorded. from a statistical point of view, all lightcurves observed at phase angles $\alpha < 30^\circ$, with peak-to-peak amplitudes $a>0.2$ mag are bimodal. second most frequently dominating harmonic was a first one, with a 3rd harmonic following right after. considering 1% of lightcurves with amplitudes $a < 0.1$ mag and phase angles $\alpha < 40^\circ$ 4th harmonic dominates.",0,0,1,0
17043,"this paper focuses on best-arm identification inside multi-armed bandits with bounded rewards. we develop an algorithm that was the fusion of lil-ucb and kl-lucb, offering a best qualities of a two algorithms inside one method. this was achieved by proving the novel anytime confidence bound considering a mean of bounded distributions, which was a analogue of a lil-type bounds recently developed considering sub-gaussian distributions. we corroborate our theoretical results with numerical experiments based on a new yorker cartoon caption contest.",0,1,0,1
15276,"lung nodule classification was the class imbalanced problem because nodules are found with much lower frequency than non-nodules. inside a class imbalanced problem, conventional classifiers tend to be overwhelmed by a majority class and ignore a minority class. we therefore propose cascaded convolutional neural networks to cope with a class imbalanced problem. inside a proposed approach, multi-stage convolutional neural networks that perform as single-sided classifiers filter out obvious non-nodules. successively, the convolutional neural network trained with the balanced data set calculates nodule probabilities. a proposed method achieved a sensitivity of 92.4\% and 94.5% at 4 and 8 false positives per scan inside free receiver operating characteristics (froc) curve analysis, respectively.",1,0,0,0
11509,"training generative adversarial networks was unstable inside high-dimensions as a true data distribution tends to be concentrated inside the small fraction of a ambient space. a discriminator was then quickly able to classify nearly all generated samples as fake, leaving a generator without meaningful gradients and causing it to deteriorate after the point inside training. inside this work, we propose training the single generator simultaneously against an array of discriminators, each of which looks at the different random low-dimensional projection of a data. individual discriminators, now provided with restricted views of a input, are unable to reject generated samples perfectly and continue to provide meaningful gradients to a generator throughout training. meanwhile, a generator learns to produce samples consistent with a full data distribution to satisfy all discriminators simultaneously. we demonstrate a practical utility of this idea behind the method experimentally, and show that it was able to produce image samples with higher quality than traditional training with the single discriminator.",1,0,0,0
8367,"word embeddings have been found to capture the surprisingly rich amount of syntactic and semantic knowledge. however, it was not yet sufficiently well-understood how a relational knowledge that was implicitly encoded inside word embeddings should be extracted inside the reliable way. inside this paper, we propose two probabilistic models to address this issue. a first model was based on a common relations-as-translations view, but was cast inside the probabilistic setting. our second model was based on a much weaker assumption that there was the linear relationship between a vector representations of related words. compared to existing approaches, our models lead to more accurate predictions, and they are more explicit about what should and cannot be extracted from a word embedding.",1,0,0,0
16535,"we present the simple method to incorporate syntactic information about a target language inside the neural machine translation system by translating into linearized, lexicalized constituency trees. an experiment on a wmt16 german-english news translation task resulted inside an improved bleu score when compared to the syntax-agnostic nmt baseline trained on a same dataset. an analysis of a translations from a syntax-aware system shows that it performs more reordering during translation inside comparison to a baseline. the small-scale human evaluation also showed an advantage to a syntax-aware system.",1,0,0,0
2393,"data-target pairing was an important step towards multi-target localization considering a intelligent operation of unmanned systems. target localization plays the crucial role inside numerous applications, such as search, and rescue missions, traffic management and surveillance. a objective of this paper was to present an innovative target location learning approach, where numerous machine learning approaches, including k-means clustering and supported vector machines (svm), are used to learn a data pattern across the list of spatially distributed sensors. to enable a accurate data association from different sensors considering accurate target localization, appropriate data pre-processing was essential, which was then followed by a application of different machine learning algorithms to appropriately group data from different sensors considering a accurate localization of multiple targets. through simulation examples, a performance of these machine learning algorithms was quantified and compared.",1,0,0,1
19641,"we present a detection of four far-infrared fine-structure oxygen lines, as well as strong upper limits considering a co(2-1) and [n ii] 205 um lines, inside 3c 368, the well-studied radio-loud galaxy at z = 1.131. these new oxygen lines, taken inside conjunction with previously observed neon and carbon fine-structure lines, suggest the powerful active galactic nucleus (agn), accompanied by vigorous and extended star formation. the starburst dominated by o8 stars, with an age of ~6.5 myr, provides the good fit to a fine-structure line data. this estimated age of a starburst makes it nearly concurrent with a latest episode of agn activity, suggesting the link between a growth of a supermassive black hole and stellar population inside this source. we do not detect a co(2-1) line, down to the level twelve times lower than a expected value considering star forming galaxies. this lack of co line emission was consistent with recent star formation activity if a star-forming molecular gas has low metallicity, was highly fractionated (such that co was photodissociated through much of a clouds), or was chemically very young (such that co has not yet had time to form). it was also possible, though we argue unlikely, that a ensemble of fine structure lines are emitted from a region heated by a agn.",0,0,1,0
14791,"regularization occurs when a output the learner produces was less variable than a linguistic data they observed. inside an artificial language learning experiment, we show that there exist at least two independent sources of regularization bias inside cognition: the domain-general source based on cognitive load and the domain-specific source triggered by linguistic stimuli. both of these factors modulate how frequency information was encoded and produced, but only a production-side modulations result inside regularization (i.e. cause learners to eliminate variation from a observed input). we formalize a definition of regularization as a reduction of entropy and find that entropy measures are better at identifying regularization behavior than frequency-based analyses. with the help of our experimental data and the model of cultural transmission, we generate predictions considering a amount of regularity that would develop inside each experimental condition if a artificial language were transmitted over several generations of learners. here we find that a effect of cognitive constraints should become more complex when put into a context of cultural evolution: although learning biases certainly carry information about a course of language evolution, we should not expect the one-to-one correspondence between a micro-level processes that regularize linguistic datasets and a macro-level evolution of linguistic regularity.",1,0,0,0
7963,"efficient stochastic simulation algorithms are of paramount importance to a study of spreading phenomena on complex networks. with the help of insights and analytical results from network science, we discuss how a structure of contacts affects a efficiency of current algorithms. we show that algorithms believed to require $\mathcal{o}(\log n)$ or even $\mathcal{o}(1)$ operations per update---where $n$ was a number of nodes---display instead the polynomial scaling considering networks that are either dense or sparse and heterogeneous. this significantly affects a required computation time considering simulations on large networks. to circumvent a issue, we propose the node-based method combined with the composition and rejection algorithm, the sampling scheme that has an average-case complexity of $\mathcal{o} [\log(\log n)]$ per update considering general networks. this systematic idea behind the method was first set-up considering markovian dynamics, but should also be adapted to the number of non-markovian processes and should enhance considerably a study of the wide range of dynamics on networks.",1,0,0,0
14175,"this article was concerned with causal structures, which are defined as the field of tangentially non-degenerate projective hypersurfaces inside a projectivized tangent bundle of the manifold. a local equivalence problem of causal structures on manifolds of dimension at least four was solved with the help of cartan's method of equivalence, leading to an $\{e\}$-structure over some principal bundle. it was shown that these structures correspond to parabolic geometries of type $(d_n,p_{1,2})$ and $(b_{n-1},p_{1,2})$, when $n\geq 4$, and $(d_3,p_{1,2,3})$. a essential local invariants are determined and interpreted geometrically. several special classes of causal structures are considered including those that are the lift of pseudo-conformal structures and those referred to as causal structures with vanishing wsf curvature. the twistorial construction considering causal structures with vanishing wsf curvature was given.",0,1,0,0
14413,"inside this work we introduce declarative statistics, the suite of declarative modelling tools considering statistical analysis. statistical constraints represent a key building block of declarative statistics. first, we introduce the range of relevant counting and matrix constraints and associated decompositions, some of which novel, that are instrumental inside a design of statistical constraints. second, we introduce the selection of novel statistical constraints and associated decompositions, which constitute the self-contained toolbox that should be used to tackle the wide range of problems typically encountered by statisticians. finally, we deploy these statistical constraints to the wide range of application areas drawn from classical statistics and we contrast our framework against established practices.",1,0,0,1
6242,"a central parsec of a milky way hosts two puzzlingly young stellar populations, the tight isotropic distribution of b stars around sgra* (the s-stars) and the disk of ob stars extending to ~0.5pc. with the help of the modified version of sverre aarseth's direct summation code nbody6 we explore a scenario inside which the young star cluster migrates to a galactic centre within a lifetime of a ob disk population using dynamical friction. we find that star clusters massive and dense enough to reach a central parsec form the very massive star using physical collisions on the mass segregation timescale. we follow a evolution of a merger product with the help of a most up to date, yet conservative, mass loss recipes considering very massive stars. over the large range of initial conditions, we find that a very massive star expels most of its mass using the strong stellar wind, eventually collapsing to form the black hole of mass 20 - 400 m_sun, incapable of bringing massive stars to a galactic centre. no massive intermediate mass black hole should form inside this scenario. a presence of the star cluster inside a central ~10 pc within a last 15 myr would also leave the ~2 pc ring of massive stars, which was not currently observed. thus, we conclude that a star cluster migration model was highly unlikely to be a origin of either young population, and in-situ formation models or binary disruptions are favoured.",0,0,1,0
10513,"chemical substitution during growth was the well-established method to manipulate electronic states of quantum materials, and leads to rich spectra of phase diagrams inside cuprate and iron-based superconductors. here we report the novel and generic strategy to achieve nonvolatile electron doping inside series of (i.e. 11 and 122 structures) fe-based superconductors by ionic liquid gating induced protonation at room temperature. accumulation of protons inside bulk compounds induces superconductivity inside a parent compounds, and enhances a tc largely inside some superconducting ones. furthermore, a existence of proton inside a lattice enables a first proton nuclear magnetic resonance (nmr) study to probe directly superconductivity. with the help of fes as the model system, our nmr study reveals an emergent high-tc phase with no coherence peak which was hard to measure by nmr with other isotopes. this novel electric-field-induced proton evolution opens up an avenue considering manipulation of competing electronic states (e.g. mott insulators), and may provide an innovative way considering the broad perspective of nmr measurements with greatly enhanced detecting resolution.",0,0,1,0
5613,"inside this paper, we study a solvability of a nonlinear dirichlet problem with sum of a operators of independent non standard growths inside the bounded domain $\omega \subset \mathbb{r}^{n}$. we obtain sufficient conditions and show a existence of weak solutions of a considered problem by with the help of monotonicity and compactness methods together.",0,1,0,0
15048,"a present paper was devoted to provide conditions considering a levi--malcev theorem to hold or not to hold (i.e. considering two levi subalgebras to be or not conjugate by an inner automorphism) inside a context of finite-dimensional leibniz algebras over the field of characteristic zero. particularly, inside a case of a field $\mathbb{c}$ of complex numbers, we consider all possible cases inside which levi subalgebras are conjugate and not conjugate.",0,1,0,0
8735,"earlier definitions of capacity considering wireless networks, e.g., transport or transmission capacity, considering which exact theoretical results are known, are well suited considering ad hoc networks but are not directly applicable considering cellular wireless networks, where large-scale basestation (bs) coordination was not possible, and retransmissions/arq under a sinr model was the universal feature. inside this paper, cellular wireless networks, where both bs locations and mobile user (mu) locations are distributed as independent poisson point processes are considered, and each mu connects to its nearest bs. with arq, under a sinr model, a effective downlink rate of packet transmission was a reciprocal of a expected delay (number of retransmissions needed till success), which we use as our network capacity definition after scaling it with a bs density. exact characterization of this natural capacity metric considering cellular wireless networks was derived. a capacity was shown to first increase polynomially with a bs density inside a low bs density regime and then scale inverse exponentially with a increasing bs density. two distinct upper bounds are derived that are relevant considering a low and a high bs density regimes. the single power control strategy was shown to achieve a upper bounds inside both a regimes. this result was fundamentally different from a well known capacity results considering ad hoc networks, such as transport and transmission capacity that scale as a square root of a (high) bs density. our results show that a strong temporal correlations of sinrs with ppp distributed bs locations was limiting, and a realizable capacity inside cellular wireless networks inside high-bs density regime was much smaller than previously thought. the byproduct of our analysis shows that a capacity of a aloha strategy with retransmissions was zero.",1,0,0,0
9597,"community structure was the commonly observed feature of real networks. a term refers to a presence inside the network of groups of nodes (communities) that feature high internal connectivity, but are poorly connected between each other. whereas a issue of community detection has been addressed inside several works, a problem of validating the partition of nodes as the good community structure considering the real network has received considerably less attention and remains an open issue. we propose the set of indices considering community structure validation of network partitions, which rely on concepts from network enrichment analysis. a proposed indices allow to compare a adequacy of different partitions of nodes as community structures. moreover, they should be employed to assess whether two networks share a same or similar community structures, and to evaluate a performance of different network clustering algorithms.",0,0,0,1
10686,"inside this paper, we exploit the memory-augmented neural network to predict accurate answers to visual questions, even when those answers occur rarely inside a training set. a memory network incorporates both internal and external memory blocks and selectively pays attention to each training exemplar. we show that memory-augmented neural networks are able to maintain the relatively long-term memory of scarce training exemplars, which was important considering visual question answering due to a heavy-tailed distribution of answers inside the general vqa setting. experimental results on two large-scale benchmark datasets show a favorable performance of a proposed algorithm with the comparison to state of a art.",1,0,0,0
12281,"biomedical text mining has become more important than ever as a number of biomedical documents rapidly grows. with a progress of machine learning, extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning was boosting a development of effective biomedical text mining models. however, as deep learning models require the large amount of training data, biomedical text mining with deep learning often fails due to a small sizes of training datasets inside biomedical fields. recent researches on learning contextualized language representation models from text corpora shed light on a possibility of leveraging the large number of unannotated biomedical text corpora. we introduce biobert (bidirectional encoder representations from transformers considering biomedical text mining), which was the domain specific language representation model pre-trained on large-scale biomedical corpora. based on a bert architecture, biobert effectively transfers a knowledge of large amount of biomedical texts into biomedical text mining models. while bert also shows competitive performances with previous state-of-the-art models, biobert significantly outperforms them on three representative biomedical text mining tasks including biomedical named entity recognition (1.86% absolute improvement), biomedical relation extraction (3.33% absolute improvement), and biomedical question answering (9.61% absolute improvement) with minimal task-specific architecture modifications. we make pre-trained weights of biobert freely available inside this https url, and source codes of fine-tuned models inside this https url.",1,0,0,0
14339,"online portfolio selection research has so far focused mainly on minimizing regret defined inside terms of wealth growth. practical financial decision making, however, was deeply concerned with both wealth and risk. we consider online learning of portfolios of stocks whose prices are governed by arbitrary (unknown) stationary and ergodic processes, where a goal was to maximize wealth while keeping a conditional value at risk (cvar) below the desired threshold. we characterize a asymptomatically optimal risk-adjusted performance and present an investment strategy whose portfolios are guaranteed to achieve a asymptotic optimal solution while fulfilling a desired risk constraint. we also numerically demonstrate and validate a viability of our method on standard datasets.",1,0,0,0
13763,"it is proven inside [b.-y. chen, f. dillen, j. van der veken and l. vrancken, curvature inequalities considering lagrangian submanifolds: a final solution, differ. geom. appl. 31 (2013), 808-819] that every lagrangian submanifold $m$ of the complex space form $\tilde m^{n}(4c)$ of constant holomorphic sectional curvature $4c$ satisfies a following optimal inequality: \begin{align*} \delta(2,n-2) \leq \frac{n^2(n-2)}{4(n-1)} h^2 + 2(n-2) c, \end{align*} where $h^2$ was a squared mean curvature and $\delta(2,n-2)$ was the $\delta$-invariant on $m$. inside this paper we classify lagrangian submanifolds of complex space forms $\tilde m^{n}(4c)$, $n \geq 5$, which satisfy a equality case of this inequality at every point.",0,1,0,0
8349,the new idea behind the method to approximate population size based on the stratified link-tracing sampling design was presented. a method extends on a frank and snijders (1994) idea behind the method by allowing considering heterogeneity inside a initial sample selection procedure. rao-blackwell estimators and corresponding resampling approximations similar to that detailed inside vincent and thompson (2017) are explored. an empirical application was provided considering the hard-to-reach networked population. a results demonstrate that a idea behind the method has much potential considering application to such populations. supplementary materials considering this article are available online.,0,0,0,1
7344,"this paper investigates a role of tutor feedback inside language learning with the help of computational models. we compare two dominant paradigms inside language learning: interactive learning and cross-situational learning - which differ primarily inside a role of social feedback such as gaze or pointing. we analyze a relationship between these two paradigms and propose the new mixed paradigm that combines a two paradigms and allows to test algorithms inside experiments that combine no feedback and social feedback. to deal with mixed feedback experiments, we develop new algorithms and show how they perform with respect to traditional knn and prototype approaches.",1,0,0,0
6938,"given the moebius homeomorphism $f : \partial x \to \partial y$ between boundaries of proper, geodesically complete cat(-1) spaces $x,y$, we describe an extension $\hat{f} : x \to y$ of $f$, called a circumcenter map of $f$, which was constructed with the help of circumcenters of expanding sets. a extension $\hat{f}$ was shown to coincide with a $(1, \log 2)$-quasi-isometric extension constructed inside [biswas3], and was locally $1/2$-holder continuous. when $x,y$ are complete, simply connected manifolds with sectional curvatures $k$ satisfying $-b^2 \leq k \leq -1$ considering some $b \geq 1$ then a extension $\hat{f} : x \to y$ was the $(1, (1 - \frac{1}{b})\log 2)$-quasi-isometry. circumcenter extension of moebius maps was natural with respect to composition with isometries.",0,1,0,0
9196,"the grand challenge of a 21st century cosmology was to accurately approximate a cosmological parameters of our universe. the major idea behind the method to estimating a cosmological parameters was to use a large-scale matter distribution of a universe. galaxy surveys provide a means to map out cosmic large-scale structure inside three dimensions. information about galaxy locations was typically summarized inside the ""single"" function of scale, such as a galaxy correlation function or power-spectrum. we show that it was possible to approximate these cosmological parameters directly from a distribution of matter. this paper presents a application of deep 3d convolutional networks to volumetric representation of dark-matter simulations as well as a results obtained with the help of the recently proposed distribution regression framework, showing that machine learning techniques are comparable to, and should sometimes outperform, maximum-likelihood point estimates with the help of ""cosmological models"". this opens a way to estimating a parameters of our universe with higher accuracy.",1,0,0,1
4967,"we present an empirical idea behind the method considering interpreting gravitational wave signals of binary black hole mergers under a assumption that a underlying black hole population was sourced by remnants of stellar evolution. with the help of a observed relationship between galaxy mass and stellar metallicity, we predict a black hole count as the function of galaxy stellar mass. we show, considering example, that the galaxy like a milky way should host millions of $\sim 30~m_\odot$ black holes and dwarf satellite galaxies like draco should host $\sim 100$ such remnants, with weak dependence on a assumed imf and stellar evolution model. most low-mass black holes ($\sim10 m_\odot$) typically reside within massive galaxies ($m_\star \simeq 10^{11} m_\odot$) while massive black holes ($\sim 50~m_\odot$) typically reside within dwarf galaxies ($m_\odot \simeq 10^9 m_\odot$) today. if roughly $1\%$ of black holes are involved inside the binary black hole merger, then a reported merger rate densities from advanced ligo should be accommodated considering the range of merger timescales, and a detection of mergers with $> 50~m_\odot$ black holes should be expected within a next decade. identifying a host galaxy population of a mergers provides the way to constrain both a binary neutron star or black hole formation efficiencies and a merger timescale distributions; these events would be primarily localized inside dwarf galaxies if a merger timescale was short compared to a age of a universe and inside massive galaxies otherwise. as more mergers are detected, a prospect of identifying a host galaxy population, either directly through a detection of electromagnetic counterparts of binary neutron star mergers or indirectly through a anisotropy of a events, will become the realistic possibility.",0,0,1,0
19179,"we consider continuous-time, finite-horizon, optimal quadratic control of semi-markov jump linear systems (s-mjls), and develop principled approximations through markov-like representations considering a holding-time distributions. we adopt the phase-type approximation considering holding times, which was known to be consistent, and translates the s-mjls into the specific mjls with partially observable modes (mjlspom), where a modes inside the cluster have a same dynamic, a same cost weighting matrices and a same control policy. considering the general mjlspom, we give necessary and sufficient conditions considering optimal (switched) linear controllers. when specialized to our particular mjlspom, we additionally establish a existence of optimal linear controller, as well as its optimality within a class of general controllers satisfying standard smoothness conditions. a known equivalence between phase-type distributions and positive linear systems allows to leverage existing modeling tools, but possibly with large computational costs. motivated by this, we propose matrix exponential approximation of holding times, resulting inside pseudo-mjlspom representation, i.e., where a transition rates could be negative. such the representation was of relatively low order, and maintains a same optimality conditions as considering a mjlspom representation, but could violate non-negativity of holding-time density functions. the two-step procedure consisting of the local pulling-up modification and the filtering technique was constructed to enforce non-negativity.",1,0,0,0
3254,"inside this paper, the multi-agent coordination problem with steady-state regulation constraints was investigated considering the class of nonlinear systems. unlike existing leader-following coordination formulations, a reference signal was not given by the dynamic autonomous leader but determined as a optimal solution of the distributed optimization problem. furthermore, we consider the global constraint having noisy data observations considering a optimization problem, which implies that reference signal was not trivially available with existing optimization algorithms. to handle those challenges, we present the passivity-based analysis and design idea behind the method by with the help of only local objective function, local data observation and exchanged information from their neighbors. a proposed distributed algorithms are shown to achieve a optimal steady-state regulation by rejecting a unknown observation disturbances considering passive nonlinear agents, which are persuasive inside various practical problems. applications and simulation examples are then given to verify a effectiveness of our design.",1,1,0,0
16367,"a second-order extended stability factorized runge-kutta-chebyshev (frkc2) class of explicit schemes considering a integration of large systems of pdes with diffusive terms was presented. frkc2 schemes are straightforward to implement through ordered sequences of forward euler steps with complex stepsizes, and easily parallelised considering large scale problems on distributed architectures. preserving 7 digits considering accuracy at 16 digit precision, a schemes are theoretically capable of maintaining internal stability at acceleration factors inside excess of 6000 with respect to standard explicit runge-kutta methods. a stability domains have approximately a same extents as those of rkc schemes, and are the third longer than those of rkl2 schemes. extension of frkc methods to fourth-order, by both complex splitting and butcher composition techniques, was discussed. the publicly available implementation of a frkc2 class of schemes may be obtained from maths.dit.ie/frkc",0,0,1,0
7133,"inverse problems, where inside broad sense a task was to learn from a noisy response about some unknown function, usually represented as a argument of some known functional form, has received wide attention inside a general scientific disciplines. how- ever, inside mainstream statistics such inverse problem paradigm does not seem to be as popular. inside this article we provide the brief overview of such problems from the statistical, particularly bayesian, perspective. we also compare and contrast a above class of problems with a perhaps more statistically familiar inverse regression problems, arguing that this class of problems contains a traditional class of inverse problems. inside course of our review we point out that a statistical literature was very scarce with respect to both a inverse paradigms, and substantial research work was still necessary to develop a fields.",0,1,0,1
10208,"we consider a problem of finding and describing minimisers of a rayleigh quotient \[ \lambda_\infty \, :=\, \inf_{u\in \mathcal{w}^{2,\infty}(\omega)\setminus\{0\} }\frac{\|\delta u\|_{l^\infty(\omega)}}{\|u\|_{l^\infty(\omega)}}, \] where $\omega \subseteq \mathbb{r}^n$ was the bounded $c^{1,1}$ domain and $\mathcal{w}^{2,\infty}(\omega)$ was the class of weakly twice differentiable functions satisfying either $u=0$ or $u=|\mathrm{d} u|=0$ on $\partial \omega$. our first main result, obtained through approximation by $l^p$-problems as $p\to \infty$, was a existence of the minimiser $u_\infty \in \mathcal{w}^{2,\infty}(\omega)$ satisfying \[ \left\{ \begin{array}{ll} \delta u_\infty \, \in \, \lambda_\infty \mathrm{sgn}(f_\infty) & \text{ a.e. inside }\omega, \\ \delta f_\infty \, =\, \mu_\infty & \text{ inside }\mathcal{d}'(\omega), \end{array} \right. \] considering some $f_\infty\in l^1(\omega)\cap bv_{\text{loc}}(\omega)$ and the measure $\mu_\infty \in \mathcal{m}(\omega)$, considering either choice of boundary conditions. here sgn was a multi-valued sign function. we also study a dependence of a eigenvalue $\lambda_\infty$ on a domain, establishing a validity of the faber-krahn type inequality: among all $c^{1,1}$ domains with fixed measure, a ball was the strict minimiser of $\omega \mapsto \lambda_\infty(\omega)$. this result was shown to hold true considering either choice of boundary conditions and inside every dimension.",0,1,0,0
10417,"we propose the new algorithm considering solving parabolic partial differential equations (pdes) and backward stochastic differential equations (bsdes) inside high dimension, by making an analogy between a bsde and reinforcement learning with a gradient of a solution playing a role of a policy function, and a loss function given by a error between a prescribed terminal condition and a solution of a bsde. a policy function was then approximated by the neural network, as was done inside deep reinforcement learning. numerical results with the help of tensorflow illustrate a efficiency and accuracy of a proposed algorithms considering several 100-dimensional nonlinear pdes from physics and finance such as a allen-cahn equation, a hamilton-jacobi-bellman equation, and the nonlinear pricing model considering financial derivatives.",1,0,0,1
12535,"off-policy learning was key to scaling up reinforcement learning as it allows to learn about the target policy from a experience generated by the different behavior policy. unfortunately, it has been challenging to combine off-policy learning with function approximation and multi-step bootstrapping inside the way that leads to both stable and efficient algorithms. inside this work, we show that a \textsc{tree backup} and \textsc{retrace} algorithms are unstable with linear function approximation, both inside theory and inside practice with specific examples. based on our analysis, we then derive stable and efficient gradient-based algorithms with the help of the quadratic convex-concave saddle-point formulation. by exploiting a problem structure proper to these algorithms, we are able to provide convergence guarantees and finite-sample bounds. a applicability of our new analysis also goes beyond \textsc{tree backup} and \textsc{retrace} and allows us to provide new convergence rates considering a gtd and gtd2 algorithms without having recourse to projections or polyak averaging.",1,0,0,0
9432,"we consider a time-dependent 2d ginzburg-landau equation inside a whole plane with terms modeling impurities and applied currents. a ginzburg-landau vortices are then subjected to three forces: their mutual repulsive coulomb-like interaction, a applied current pushing them inside the fixed direction, and a pinning force attracting them towards a impurities. a competition between a three was expected to lead to complicated glassy effects. we rigorously study a limit inside which a number of vortices $n_\epsilon$ blows up as a inverse ginzburg-landau parameter $\epsilon$ goes to $0$, and we derive using the modulated energy method fluid-like mean-field evolution equations. these results hold considering parabolic, conservative, and mixed-flow dynamics inside appropriate regimes of $n_\epsilon\uparrow\infty$. finally, we briefly discuss some natural homogenization questions raised by this study.",0,1,0,0
15448,"considering hidden markov models one of a most popular estimates of a hidden chain was a viterbi path -- a path maximising a posterior probability. we consider the more general setting, called a pairwise markov model, where a joint process consisting of finite-state hidden regime and observation process was assumed to be the markov chain. we prove that under some conditions it was possible to extend a viterbi path to infinity considering almost every observation sequence which inside turn enables to define an infinite viterbi decoding of a observation process, called a viterbi process. this was done by constructing the block of observations, called the barrier, which ensures that a viterbi path goes trough the given state whenever this block occurs inside a observation sequence.",0,1,0,1
16002,"we consider a problem of repeatedly solving the variant of a same dynamic programming problem inside successive trials. an instance of a type of problems we consider was to find the good binary search tree inside the changing environment.at a beginning of each trial, a learner probabilistically chooses the tree with a $n$ keys at a internal nodes and a $n+1$ gaps between keys at a leaves. a learner was then told a frequencies of a keys and gaps and was charged by a average search cost considering a chosen tree. a problem was online because a frequencies should change between trials. a goal was to develop algorithms with a property that their total average search cost (loss) inside all trials was close to a total loss of a best tree chosen inside hindsight considering all trials. a challenge, of course, was that a algorithm has to deal with exponential number of trees. we develop the general methodology considering tackling such problems considering the wide class of dynamic programming algorithms. our framework allows us to extend online learning algorithms like hedge and component hedge to the significantly wider class of combinatorial objects than is possible before.",1,0,0,0
17208,"we introduce the hybrid metaphor considering a visualization of a reconciliations of co-phylogenetic trees, that are mappings among a nodes of two trees. a typical application was a visualization of a co-evolution of hosts and parasites inside biology. our strategy combines the space-filling and the node-link approach. differently from traditional methods, it guarantees an unambiguous and `downward' representation whenever a reconciliation was time-consistent (i.e., meaningful). we address a problem of a minimization of a number of crossings inside a representation, by giving the characterization of planar instances and by establishing a complexity of a problem. finally, we propose heuristics considering computing representations with few crossings.",1,0,0,0
1792,"inside this paper we present solutions to three short comings of smoothed particles hydrodynamics (sph) encountered inside previous work when applying it to giant impacts. first we introduce the novel method to obtain accurate sph representations of the planet's equilibrium initial conditions based on equal area tessellations of a sphere. this allows one to imprint an arbitrary density and internal energy profile with very low noise which substantially reduces computation because these models require no relaxation prior to use. as the consequence one should significantly increase a resolution and more flexibly change a initial bodies to explore larger parts of a impact parameter space inside simulations. a second issue addressed was a proper treatment of a matter/vacuum boundary at the planet's surface with the modified sph density estimator that properly calculates a density stabilizing a models and avoiding an artificially low density atmosphere prior to impact. further we present the novel sph scheme that simultaneously conserves both energy and entropy considering an arbitrary equation of state. this prevents loss of entropy during a simulation and further assures that a material does not evolve into unphysical states. application of these modifications to impact simulations considering different resolutions up to $6.4 \cdot 10^6$ particles show the general agreement with prior result. however, we observe resolution dependent differences inside a evolution and composition of post collision ejecta. this strongly suggests that a use of more sophisticated equations of state also demands the large number of particles inside such simulations.",0,0,1,0
15168,"statistical inference should be computationally prohibitive inside ultrahigh-dimensional linear models. correlation-based variable screening, inside which one leverages marginal correlations considering removal of irrelevant variables from a model prior to statistical inference, should be used to overcome this challenge. prior works on correlation-based variable screening either impose strong statistical priors on a linear model or assume specific post-screening inference methods. this paper first extends a analysis of correlation-based variable screening to arbitrary linear models and post-screening inference techniques. inside particular, ($i$) it shows that the condition---termed a screening condition---is sufficient considering successful correlation-based screening of linear models, and ($ii$) it provides insights into a dependence of marginal correlation-based screening on different problem parameters. numerical experiments confirm that these insights are not mere artifacts of analysis; rather, they are reflective of a challenges associated with marginal correlation-based variable screening. second, a paper explicitly derives a screening condition considering two families of linear models, namely, sub-gaussian linear models and arbitrary (random or deterministic) linear models. inside a process, it establishes that---under appropriate conditions---it was possible to reduce a dimension of an ultrahigh-dimensional, arbitrary linear model to almost a sample size even when a number of active variables scales almost linearly with a sample size.",0,1,0,1
11256,"the networked control system (ncs) consisting of cascaded two-port communication channels between a plant and controller was modeled and analyzed. towards this end, a robust stability of the standard closed-loop system inside a presence of conelike perturbations on a system graphs was investigated. a underlying geometric insights are then exploited to analyze a two-port ncs. it was shown that a robust stability of a two-port ncs should be guaranteed when a nonlinear uncertainties inside a transmission matrices are sufficiently small inside norm. a stability condition, given inside a form of ""arcsin"" of a uncertainty bounds, was both necessary and sufficient.",1,0,0,0
16422,"let $p\geq 5$ be the prime number, $g$ the split connected reductive group defined over the $p$-adic field, and $i_1$ the choice of pro-$p$-iwahori subgroup. let $c$ be an algebraically closed field of characteristic $p$ and $\mathcal{h}$ a pro-$p$-iwahori--hecke algebra over $c$ associated to $i_1$. inside this note, we compute a action of $\mathcal{h}$ on $\textrm{h}^1(i_1,c)$ and $\textrm{h}^{\textrm{top}}(i_1,c)$ when a root system of $g$ was irreducible. we also give some partial results inside a general case.",0,1,0,0
18939,"high dimensional piecewise stationary graphical models represent the versatile class considering modelling time varying networks arising inside diverse application areas, including biology, economics, and social sciences. there has been recent work inside offline detection and approximation of regime changes inside a topology of sparse graphical models. however, a online setting remains largely unexplored, despite its high relevance to applications inside sensor networks and other engineering monitoring systems, as well as financial markets. to that end, this work introduces the novel scalable online algorithm considering detecting an unknown number of abrupt changes inside a inverse covariance matrix of sparse gaussian graphical models with small delay. a proposed algorithm was based upon monitoring a conditional log-likelihood of all nodes inside a network and should be extended to the large class of continuous and discrete graphical models. we also investigate asymptotic properties of our procedure under certain mild regularity conditions on a graph size, sparsity level, number of samples, and pre- and post-changes inside a topology of a network. numerical works on both synthetic and real data illustrate a good performance of a proposed methodology both inside terms of computational and statistical efficiency across numerous experimental settings.",0,0,0,1
18108,"we argue that a standard graph laplacian was preferable considering spectral partitioning of signed graphs compared to a signed laplacian. simple examples demonstrate that partitioning based on signs of components of a leading eigenvectors of a signed laplacian may be meaningless, inside contrast to partitioning based on a fiedler vector of a standard graph laplacian considering signed graphs. we observe that negative eigenvalues are beneficial considering spectral partitioning of signed graphs, making a fiedler vector easier to compute.",1,0,0,1
744,"a world health organization (who) reported 1.25 million deaths yearly due to road traffic accidents worldwide and a number has been continuously increasing over a last few years. nearly fifth of these accidents are caused by distracted drivers. existing work of distracted driver detection was concerned with the small set of distractions (mostly, cell phone usage). unreliable ad-hoc methods are often used.in this paper, we present a first publicly available dataset considering driver distraction identification with more distraction postures than existing alternatives. inside addition, we propose the reliable deep learning-based solution that achieves the 90% accuracy. a system consists of the genetically-weighted ensemble of convolutional neural networks, we show that the weighted ensemble of classifiers with the help of the genetic algorithm yields inside the better classification confidence. we also study a effect of different visual elements inside distraction detection by means of face and hand localizations, and skin segmentation. finally, we present the thinned version of our ensemble that could achieve 84.64% classification accuracy and operate inside the real-time environment.",1,0,0,1
9682,"recurrent neural networks (rnn) are widely used to solve the variety of problems and as a quantity of data and a amount of available compute have increased, so have model sizes. a number of parameters inside recent state-of-the-art networks makes them hard to deploy, especially on mobile phones and embedded devices. a challenge was due to both a size of a model and a time it takes to evaluate it. inside order to deploy these rnns efficiently, we propose the technique to reduce a parameters of the network by pruning weights during a initial training of a network. at a end of training, a parameters of a network are sparse while accuracy was still close to a original dense neural network. a network size was reduced by 8x and a time required to train a model remains constant. additionally, we should prune the larger dense network to achieve better than baseline performance while still reducing a total number of parameters significantly. pruning rnns reduces a size of a model and should also aid achieve significant inference time speed-up with the help of sparse matrix multiply. benchmarks show that with the help of our technique model size should be reduced by 90% and speed-up was around 2x to 7x.",1,0,0,0
16109,"very recently proximal policy optimization (ppo) algorithms have been proposed as first-order optimization methods considering effective reinforcement learning. while ppo was inspired by a same learning theory that justifies trust region policy optimization (trpo), ppo substantially simplifies algorithm design and improves data efficiency by performing multiple epochs of \emph{clipped policy optimization} from sampled data. although clipping inside ppo stands considering an important new mechanism considering efficient and reliable policy update, it may fail to adaptively improve learning performance inside accordance with a importance of each sampled state. to address this issue, the new surrogate learning objective featuring an adaptive clipping mechanism was proposed inside this paper, enabling us to develop the new algorithm, known as ppo-$\lambda$. ppo-$\lambda$ optimizes policies repeatedly based on the theoretical target considering adaptive policy improvement. meanwhile, destructively large policy update should be effectively prevented through both clipping and adaptive control of the hyperparameter $\lambda$ inside ppo-$\lambda$, ensuring high learning reliability. ppo-$\lambda$ enjoys a same simple and efficient design as ppo. empirically on several atari game playing tasks and benchmark control tasks, ppo-$\lambda$ also achieved clearly better performance than ppo.",0,0,0,1
8809,"inside this paper we describe the deep learning system that has been designed and built considering a wassa 2017 emotion intensity shared task. we introduce the representation learning idea behind the method based on inner attention on top of an rnn. results show that our model offers good capabilities and was able to successfully identify emotion-bearing words to predict intensity without leveraging on lexicons, obtaining a 13th place among 22 shared task competitors.",1,0,0,0
8462,"we propose the representation of a indian summer monsoon rainfall inside terms of the probabilistic model based on the markov random field, consisting of discrete state variables representing low and high rainfall at grid-scale and daily rainfall patterns across space and inside time. these discrete states are conditioned on observed daily gridded rainfall data from a period 2000-2007. a model gives us the set of 10 spatial patterns of daily monsoon rainfall over india, which are robust over the range of user-chosen parameters as well as coherent inside space and time. each day inside a monsoon season was assigned precisely one of a spatial patterns, that approximates a spatial distribution of rainfall on that day. such approximations are quite accurate considering nearly 95% of a days. remarkably, these patterns are representative (with similar accuracy) of a monsoon seasons from 1901 to 2000 as well. finally, we compare a proposed model with alternative approaches to extract spatial patterns of rainfall, with the help of empirical orthogonal functions as well as clustering algorithms such as k-means and spectral clustering.",0,0,0,1
1830,"we present a highest spatial resolution alma observations to date of a class i protostar wl 17 inside a $\rho$ ophiuchus l1688 molecular cloud complex, which show that it has the 12 au hole inside a center of its disk. we consider whether wl 17 was actually the class ii disk being extincted by foreground material, but find that such models do not provide the good fit to a broadband sed and also require such high extinction that it would presumably arise from dense material close to a source such as the remnant envelope. self-consistent models of the disk embedded inside the rotating collapsing envelope should nicely reproduce both a alma 3 mm observations and a broadband sed of wl 17. this suggests that wl 17 was the disk inside a early stages of its formation, and yet even at this young age a inner disk has been depleted. although there are multiple pathways considering such the hole to be created inside the disk, if this hole were produced by a formation of planets it could place constraints on a timescale considering a growth of planetesimals inside protoplanetary disks.",0,0,1,0
13764,"complex systems should be modelled at various levels of detail. ideally, causal models of a same system should be consistent with one another inside a sense that they agree inside their predictions of a effects of interventions. we formalise this notion of consistency inside a case of structural equation models (sems) by introducing exact transformations between sems. this provides the general language to consider, considering instance, a different levels of description inside a following three scenarios: (a) models with large numbers of variables versus models inside which a `irrelevant' or unobservable variables have been marginalised out; (b) micro-level models versus macro-level models inside which a macro-variables are aggregate features of a micro-variables; (c) dynamical time series models versus models of their stationary behaviour. our analysis stresses a importance of well specified interventions inside a causal modelling process and sheds light on a interpretation of cyclic sems.",1,0,0,1
7922,"this paper was concerned with the discrete-time mean-field stochastic linear-quadratic optimal control problem arose from financial application. through matrix dynamical optimization method, the group of linear feedback controls was investigated. a problem was then reformulated as an operator stochastic linear-quadratic optimal control problem by the sequence of bounded linear operators over hilbert space, a optimal control with six algebraic riccati difference equations was obtained by backward induction. a two above approaches are proved to be coincided by a classical method of completing a square. finally, after discussing a solution of a problem under multidimensional noises, the financial application example was given.",0,1,0,0
13244,"it was the great pleasure to be invited to join a chorus on this auspicious occasion to celebrate professor k. alex mueller's 90th birthday by professors annette bussman-holder, hugo keller, and antonio bianconi. as the student inside high temperature superconductivity, i am forever grateful to professor alex mueller and dr. georg bednorz ""for their important breakthrough inside a discovery of superconductivity inside a ceramic materials"" inside 1986 as described inside a citation of their 1987 nobel prize inside physics. it was this breakthrough discovery that has ushered inside a explosion of research activities inside high temperature superconductivity (hts) and has provided immense excitement inside hts science and technology inside a ensuing decades till now. alex has not been resting on his laurels and has continued to search considering a origin of a unusual high temperature superconductivity inside cuprates.",0,0,1,0
8392,"we evaluate a performance of four machine learning methods considering modeling and predicting fcc solute diffusion barriers. more than 200 fcc solute diffusion barriers from previous density functional theory (dft) calculations served as our dataset to train four machine learning methods: linear regression (lr), decision tree (dt), gaussian kernel ridge regression (gkrr), and artificial neural network (ann). we separately optimize key physical descriptors favored by each method to model diffusion barriers. we also assess a ability of each method to extrapolate when faced with new hosts with limited known data. gkrr and ann were found to perform a best, showing 0.15 ev cross-validation errors and predicting impurity diffusion inside new hosts to within 0.2 ev when given only 5 data points from a host. we demonstrate a success of the combined dft + data mining idea behind the method towards solving materials science challenges and predict a diffusion barrier of all available impurities across all fcc hosts.",0,0,1,0
646,"we present the new preprocessing algorithm considering embedding a nodes of the given edge-weighted undirected graph into the euclidean space. a euclidean distance between any two nodes inside this space approximates a length of a shortest path between them inside a given graph. later, at runtime, the shortest path between any two nodes should be computed with a* search with the help of a euclidean distances as heuristic. our preprocessing algorithm, called fastmap, was inspired by a data mining algorithm of a same name and runs inside near-linear time. hence, fastmap was orders of magnitude faster than competing approaches that produce the euclidean embedding with the help of semidefinite programming. fastmap also produces admissible and consistent heuristics and therefore guarantees a generation of shortest paths. moreover, fastmap applies to general undirected graphs considering which many traditional heuristics, such as a manhattan distance heuristic, are not well defined. empirically, we demonstrate that a* search with the help of a fastmap heuristic was competitive with a* search with the help of other state-of-the-art heuristics, such as a differential heuristic.",1,0,0,0
8656,"we introduce the multiple testing procedure (treebh) which addresses a challenge of controlling error rates at multiple levels of resolution. conceptually, we frame this problem as a selection of hypotheses which are organized hierarchically inside the tree structure. we describe the fast algorithm considering a proposed sequential procedure, and prove that it controls relevant error rates given certain assumptions on a dependence among a p-values. through simulations, we demonstrate that treebh offers a desired guarantees under the range of dependency structures (including one similar to that encountered inside genome-wide association studies) and that it has a potential of gaining power over alternative methods. we also introduce the modified version of treebh which we prove to control a relevant error rates under any dependency structure. we conclude with two case studies: we first analyze data collected as part of a genotype-tissue expression (gtex) project, which aims to characterize a genetic regulation of gene expression across multiple tissues inside a human body, and secondly, data examining a relationship between a gut microbiome and colorectal cancer.",0,0,0,1
8221,we present measurements of a frequency transfer stability and analysis of a noise characteristics of an optical signal propagating over aerial suspended fiber links up to 153.6 km inside length. a measured frequency transfer stability over these links was on a order of 10^-11 at an integration time of one second dropping to 10^-12 considering integration times longer than 100 s. we show that wind-loading of a cable spans was a dominant source of short-timescale noise on a fiber links. we also report an attempt to stabilize a optical frequency transfer over these aerial links.,0,0,1,0
17817,"pca was the classical statistical technique whose simplicity and maturity has seen it find widespread use as an anomaly detection technique. however, it was limited inside this regard by being sensitive to gross perturbations of a input, and by seeking the linear subspace that captures normal behaviour. a first issue has been dealt with by robust pca, the variant of pca that explicitly allows considering some data points to be arbitrarily corrupted, however, this does not resolve a second issue, and indeed introduces a new issue that one should no longer inductively find anomalies on the test set. this paper addresses both issues inside the single model, a robust autoencoder. this method learns the nonlinear subspace that captures a majority of data points, while allowing considering some data to have arbitrary corruption. a model was simple to train and leverages recent advances inside a optimisation of deep neural networks. experiments on the range of real-world datasets highlight a model's effectiveness.",1,0,0,1
15613,"we study a dependence of galaxy clustering on atomic gas mass with the help of the sample of $\sim$16,000 galaxies with redshift inside a range of $0.0025<z<0.05$ and hi mass of $m_{\rm hi}>10^8m_{\odot}$, drawn from a 70% complete sample of a arecibo legacy fast alfa survey. we construct subsamples of galaxies with $m_{\rm hi}$ above different thresholds, and make volume-limited clustering measurements inside terms of three statistics: a projected two-point correlation function, a projected cross-correlation function with respect to the reference sample selected from a sloan digital sky survey, and a redshift-space monopole moment. inside contrast to previous studies, which found no/weak hi-mass dependence, we find both a clustering amplitude on scales above the few mpc and a bias factors to increase significantly with increasing hi mass considering subsamples with hi mass thresholds above $10^9m_{\odot}$. considering hi mass thresholds below $10^9m_{\odot}$, while a measurements have large uncertainties caused by a limited survey volume and sample size, a inferred galaxy bias factors are systematically lower than a minimum halo bias factor from mass-selected halo samples. a simple halo model, inside which galaxy content was only determined by halo mass, has difficulties inside interpreting a clustering measurements of a hi-selected samples. we extend a simple model by including a halo formation time as an additional parameter. the model that puts hi-rich galaxies into halos that formed late should reproduce a clustering measurements reasonably well. we present a implications of our best-fitting model on a correlation of hi mass with halo mass and formation time, as well as a halo occupation distributions and hi mass functions considering central and satellite galaxies. these results are compared with a predictions from semi-analytic galaxy formation models and hydrodynamic galaxy formation simulations.",0,0,1,0
293,"a paper provides global optimization algorithms considering two particularly difficult nonconvex problems raised by hybrid system identification: switching linear regression and bounded-error estimation. while most works focus on local optimization heuristics without global optimality guarantees or with guarantees valid only under restrictive conditions, a proposed idea behind the method always yields the solution with the certificate of global optimality. this idea behind the method relies on the branch-and-bound strategy considering which we devise lower bounds that should be efficiently computed. inside order to obtain scalable algorithms with respect to a number of data, we directly optimize a model parameters inside the continuous optimization setting without involving integer variables. numerical experiments show that a proposed algorithms offer the higher accuracy than convex relaxations with the reasonable computational burden considering hybrid system identification. inside addition, we discuss how bounded-error approximation was related to robust approximation inside a presence of outliers and exact recovery under sparse noise, considering which we also obtain promising numerical results.",1,0,0,1
8285,we consider an arbitrary algebra of a class of brauer configuration algebras and calculate a dimension of a center by determining the $k$-basis.,0,1,0,0
6280,"deep artificial neural networks require the large corpus of training data inside order to effectively learn, where collection of such training data was often expensive and laborious. data augmentation overcomes this issue by artificially inflating a training set with label preserving transformations. recently there has been extensive use of generic data augmentation to improve convolutional neural network (cnn) task performance. this study benchmarks various popular data augmentation schemes to allow researchers to make informed decisions as to which training methods are most appropriate considering their data sets. various geometric and photometric schemes are evaluated on the coarse-grained data set with the help of the relatively simple cnn. experimental results, run with the help of 4-fold cross-validation and reported inside terms of top-1 and top-5 accuracy, indicate that cropping inside geometric augmentation significantly increases cnn task performance.",1,0,0,1
478,"a discovery of $\gamma$-ray emission from radio-loud narrow-line seyfert 1 (nlsy1) galaxies has questioned a need considering large black hole masses (> 10$^8$ m$_{\odot}$) to launch relativistic jets. we present near-infrared data of a $\gamma$-ray-emitting nlsy1 fbqs j1644+2619 that were collected with the help of a camera circe (canarias infrared camera experiment) at a 10.4-m gran telescopio canarias to investigate a structural properties of its host galaxy and to infer a black hole mass. a 2d surface brightness profile was modelled by a combination of the nuclear and the bulge component with the s?rsic profile with index $n$ = 3.7, indicative of an elliptical galaxy. a structural parameters of a host are consistent with a correlations of effective radius and surface brightness against absolute magnitude measured considering elliptical galaxies. from a bulge luminosity, we estimated the black hole mass of (2.1$\pm$0.2) $\times$10$^8$ m$_{\odot}$, consistent with a values characterizing radio-loud active galactic nuclei.",0,0,1,0
11536,"ultrafast laser excitation of the metal causes correlated, highly nonequilibrium dynamics of electronic and ionic degrees of freedom, which are however only poorly captured by a widely-used two-temperature model. here we develop an out-of-equilibrium theory that captures a full dynamic evolution of a electronic and phononic populations and provides the microscopic description of a transfer of energy delivered optically into electrons to a lattice. all essential nonequilibrium energy processes, such as electron-phonon and phonon-phonon interactions are taken into account. moreover, as all required quantities are obtained from first-principles calculations, a model gives an exact description of a relaxation dynamics without a need considering fitted parameters. we apply a model to fept and show that a detailed relaxation was out-of-equilibrium considering picoseconds.",0,0,1,0
12042,"a social phenomenon of familiar strangers is identified by stanley milgram inside 1972 with the small-scale experiment. however, there has been limited research focusing on uncovering a phenomenon at the societal scale and simultaneously investigating a social relationships between familiar strangers. with a aid of a large-scale mobile phone records, we empirically show a existence of a relationship inside a country of andorra. built upon a temporal and spatial distributions, we investigate a mechanisms, especially collective temporal regularity and spatial structure that trigger this phenomenon. moreover, we explore a relationship between social distances on a communication network and a number of encounters and show that larger number of encounters indicates shorter social distances inside the social network. a understanding of a physical encounter network could have important implications to understand a phenomena such as epidemics spreading and information diffusion.",1,0,0,0
7774,"imaging atmospheric cherenkov telescopes (iacts) represent the class of instruments which are dedicated to a ground-based observation of cosmic vhe gamma ray emission based on a detection of a cherenkov radiation produced inside a interaction of gamma rays with a earth atmosphere. one of a key elements of such instruments was the pixelized focal-plane camera consisting of photodetectors. to date, photomultiplier tubes (pmts) have been a common choice given their high photon detection efficiency (pde) and fast time response. recently, silicon photomultipliers (sipms) are emerging as an alternative. this rapidly evolving technology has strong potential to become superior to that based on pmts inside terms of pde, which would further improve a sensitivity of iacts, and see the price reduction per square millimeter of detector area. we are working to develop the sipm-based module considering a focal-plane cameras of a magic telescopes to probe this technology considering iacts with large focal plane cameras of an area of few square meters. we will describe a solutions we are exploring inside order to balance the competitive performance with the minimal impact on a overall magic camera design with the help of ray tracing simulations. we further present the comparative study of a overall light throughput based on monte carlo simulations and considering a properties of a major hardware elements of an iact.",0,0,1,0
10983,"we consider inside this paper a regularity problem considering time-optimal trajectories of the single-input control-affine system on the n-dimensional manifold. we prove that, under generic conditions on a drift and a controlled vector field, any control u associated with an optimal trajectory was smooth out of the countable set of times. more precisely, there exists an integer k, only depending on a dimension n, such that a non-smoothness set of u was made of isolated points, accumulations of isolated points, and so on up to k-th order iterated accumulations.",0,1,0,0
6105,"inside this paper, we introduce new classes of divergences by extending a definitions of a bregman divergence and a skew jensen divergence. these new divergence classes (g-bregman divergence and skew g-jensen divergence) satisfy some properties similar to a bregman or skew jensen divergence. we show these g-divergences include divergences which belong to the class of f-divergence (the hellinger distance, a chi-square divergence and a alpha-divergence inside addition to a kullback-leibler divergence). moreover, we derive an inequality between a g-bregman divergence and a skew g-jensen divergence and show this inequality was the generalization of lin's inequality.",0,0,0,1
7075,"density functional theory and nonequilibrium green's function calculations have been used to explore spin-resolved transport through a high-spin state of an iron(ii)sulfur single molecular magnet. our results show that this molecule exhibits near-perfect spin filtering, where a spin-filtering efficiency was above 99%, as well as significant negative differential resistance centered at the low bias voltage. a rise inside a spin-up conductivity up to a bias voltage of 0.4 v was dominated by the conductive lowest unoccupied molecular orbital, and this was accompanied by the slight increase inside a magnetic moment of a fe atom. a subsequent drop inside a spin-up conductivity was because a conductive channel moves to a highest occupied molecular orbital which has the lower conductance contribution. this was accompanied by the drop inside a magnetic moment of a fe atom. these two exceptional properties, and a fact that a onset of negative differential resistance occurs at low bias voltage, suggests a potential of a molecule inside nanoelectronic and nanospintronic applications.",0,0,1,0
4081,"controlling quasiparticle dynamics should improve a performance of superconducting devices. considering example, it has been demonstrated effective inside increasing lifetime and stability of superconducting qubits. here we study how to optimize a placement of normal-metal traps inside transmon-type qubits. when a trap size increases beyond the certain characteristic length, a details of a geometry and trap position, and even a number of traps, become important. we discuss considering some experimentally relevant examples how to shorten a decay time of a excess quasiparticle density. moreover, we show that the trap inside a vicinity of the josephson junction should reduce a steady-state quasiparticle density near that junction, thus suppressing a quasiparticle-induced relaxation rate of a qubit. such the trap also reduces a impact of fluctuations inside a generation rate of quasiparticles, rendering a qubit more stable.",0,0,1,0
19336,"recent transport experiments inside a cuprate superconductors linked a opening of a pseudogap to the change inside electronic dispersion [s. badoux et al., nature 531, 210 (2015)]. transport measurements showed that a carrier density sharply changes from $x$ to $1+x$ at a pseudogap critical doping, inside accordance with a change from fermi arcs at low doping to the large hole fermi surface at high doping. a su(2) theory of cuprates shows that antiferromagnetic short range interactions cause a arising of both charge and superconducting orders, which are related by an su(2) symmetry. a fluctuations associated with this symmetry form the pseudogap phase. here we derive a renormalised electronic propagator under a su(2) dome, and calculate a spectral functions and transport quantities of a renormalised bands. we show that their evolution with doping matches both spectral and transport measurements.",0,0,1,0
2822,"the classical and useful way to study controllability problems was a moment method developed by fattorini-russell, based on a construction of suitable biorthogonal families. several recent problems exhibit a same behaviour: a eigenvalues of a problem satisfy the uniform but rather 'bad' gap condition, and the rather 'good' but only asymptotic one. a goal of this work was to obtain general and precise upper and lower bounds considering biorthogonal families under these two gap conditions, and so to measure a influence of a 'bad' gap condition and a good influence of a 'good' asymptotic one. to achieve our goals, we extend some of a general results of fattorini-russell concerning biorthogonal families, with the help of complex analysis techniques developed by seidman, g?ichal, tenenbaum-tucsnak, and lissy.",0,1,0,0
16307,"internet protocol (ip) addresses are frequently used as the method of locating web users by researchers inside several different fields. however, there are competing reports concerning a accuracy of those locations, and little research has been done inside manually comparing a ip geolocation databases and web page geographic information. this paper categorized web page from a yahoo search engine into twelve categories, ranging from 'blog' and 'news' to 'education' and 'governmental'. then we manually compared a mailing or street address of a web page's content creator with a geolocation results by a given ip address. we introduced the cartographic design method by creating kernel density maps considering visualizing a information landscape of web pages associated with specific keywords.",1,0,0,0
4335,"we find evidence that a newly discovered fe-based superconductor kca$_2$fe$_4$as$_4$f$_2$ ($t_c~=~33.36(7)$~k) displays multigap superconductivity with line nodes. transverse field muon spin rotation ($\mu$sr) measurements show that a temperature dependence of a superfluid density does not have a expected behavior of the fully-gapped superconductor, due to a lack of saturation at low temperatures. moreover, a data cannot be well fitted with the help of either single band models or the multiband $s$-wave model, yet are well described by two-gap models with line nodes on either one or both of a gaps. meanwhile a zero-field $\mu$sr results indicate the lack of time reversal symmetry breaking inside a superconducting state, but suggest a presence of magnetic fluctuations. these results demonstrate the different route considering realizing nodal superconductivity inside iron-based superconductors. here a gap structure was drastically altered upon replacing one of a spacer layers, indicating a need to understand how a pairing state was tuned by changes of a asymmetry between a pnictogens located either side of a fe planes.",0,0,1,0
2833,"galaxy evolution should be studied observationally by linking progenitor and descendant galaxies through an evolving cumulative number density selection. this procedure should reproduce a expected evolution of a median stellar mass from abundance matching. however, models predict an increasing scatter inside main progenitor masses at higher redshifts, which makes galaxy selection at a median mass unrepresentative. consequently, there was no guarantee that a evolution of other galaxy properties deduced from this selection are reliable. despite this concern, we show that this procedure approximately reproduces a evolution of a average stellar density profile of main progenitors of m = 10^11.5 msun galaxies, when applied to a eagle hydrodynamical simulation. at z > 3.5 a aperture masses disagree by about the factor two, but this discrepancy disappears when we include a expected scatter inside cumulative number densities. a evolution of a average density profile inside eagle broadly agrees with observations from ultravista and candels, suggesting an inside-out growth history considering these massive galaxies over 0 < z < 5. however, considering z < 2 a inside-out growth trend was stronger inside eagle. we conclude that cumulative number density matching gives reasonably accurate results when applied to a evolution of a mean density profile of massive galaxies.",0,0,1,0
4412,"music was usually highly structured and it was still an open question how to design models which should successfully learn to recognize and represent musical structure. the fundamental problem was that structurally related patterns should have very distinct appearances, because a structural relationships are often based on transformations of musical material, like chromatic or diatonic transposition, inversion, retrograde, or rhythm change. inside this preliminary work, we study a potential of two unsupervised learning techniques - restricted boltzmann machines (rbms) and gated autoencoders (gaes) - to capture pre-defined transformations from constructed data pairs. we evaluate a models by with the help of a learned representations as inputs inside the discriminative task where considering the given type of transformation (e.g. diatonic transposition), a specific relation between two musical patterns must be recognized (e.g. an upward transposition of diatonic steps). furthermore, we measure a reconstruction error of models when reconstructing musical transformed patterns. lastly, we test a models inside an analogy-making task. we find that it was difficult to learn musical transformations with a rbm and that a gae was much more adequate considering this task, since it was able to learn representations of specific transformations that are largely content-invariant. we believe these results show that models such as gaes may provide a basis considering more encompassing music analysis systems, by endowing them with the better understanding of a structures underlying music.",1,0,0,0
2441,"monolayer tellurium (te) or tellurene has been suggested by the recent theory as the new two-dimensional (2d) system with great electronic and optoelectronic promises. here we present an experimental study of epitaxial te deposited on highly oriented pyrolytic graphite (hopg) substrate by molecular-beam epitaxy. scanning tunneling microscopy of ultrathin layers of te reveals rectangular surface cells with a cell size consistent with a theoretically predicted beta-tellurene, whereas considering thicker films, a cell size was more consistent with that of a (10-10) surface of bulk te crystal. scanning tunneling spectroscopy measurements show a films are semiconductors with a energy bandgaps decreasing with increasing film thickness, and a gap narrowing occurs predominantly at a valance-band maximum (vbm). a latter was understood by strong coupling of states at a vbm but the weak coupling at conduction band minimum (cbm) as revealed by density functional theory calculations.",0,0,1,0
9971,"eos family is created during the catastrophic impact about 1.3 gyr ago. rotation states of individual family members contain information about a history of a whole population. we aim to increase a number of asteroid shape models and rotation states within a eos collision family, as well as to revise previously published shape models from a literature. such results should be used to constrain theoretical collisional and evolution models of a family, or to approximate other physical parameters by the thermophysical modeling of a thermal infrared data. we use all available disk-integrated optical data (i.e., classical dense-in-time photometry obtained from public databases and through the large collaboration network as well as sparse-in-time individual measurements from the few sky surveys) as input considering a convex inversion method, and derive 3d shape models of asteroids together with their rotation periods and orientations of rotation axes. we present updated shape models considering 15 asteroids and new shape model determinations considering 16 asteroids. together with a already published models from a publicly available damit database, we compiled the sample of 56 eos family members with known shape models that we used inside our analysis of physical properties within a family. rotation states of asteroids smaller than ~20 km are heavily influenced by a yorp effect, whilst a large objects more or less retained their rotation state properties since a family creation. moreover, we also present the shape model and bulk density of asteroid (423) diotima, an interloper inside a eos family, based on a disk-resolved data obtained by a near infrared camera (nirc2) mounted on a w.m. keck ii telescope.",0,0,1,0
15072,"we consider a paradigm of the black box ai system that makes life-critical decisions. we propose an ""arguing machines"" framework that pairs a primary ai system with the secondary one that was independently trained to perform a same task. we show that disagreement between a two systems, without any knowledge of underlying system design or operation, was sufficient to arbitrarily improve a accuracy of a overall decision pipeline given human supervision over disagreements. we demonstrate this system inside two applications: (1) an illustrative example of image classification and (2) on large-scale real-world semi-autonomous driving data. considering a first application, we apply this framework to image classification achieving the reduction from 8.0% to 2.8% top-5 error on imagenet. considering a second application, we apply this framework to tesla autopilot and demonstrate a ability to predict 90.4% of system disengagements that were labeled by human annotators as challenging and needing human supervision.",1,0,0,0
3643,"when a stratospheric observatory considering infrared astronomy (sofia) is conceived and its first science cases defined, exoplanets had not been detected. later studies, however, showed that optical and near-infrared photometric and spectrophotometric follow-up observations during planetary transits and eclipses are feasible with sofia's instrumentation, inside particular with a hipo-flitecam and fpi+ optical and near infrared (nir) instruments. additionally, a airborne-based platform sofia has the number of unique advantages when compared to other ground- and space-based observatories inside this field of research. here we will outline these theoretical advantages, present some sample science cases and a results of two observations from sofia's first five observation cycles -- an observation of a hot jupiter hd 189733b with hipo and an observation of a super-earth gj 1214b with flipo and fpi+. based on these early products available to this science case, we evaluate sofia's potential and future perspectives inside a field of optical and infrared exoplanet spectrophotometry inside a stratosphere.",0,0,1,0
774,"we propose an orthogonal series density estimator considering complex surveys, where samples are neither independent nor identically distributed. a proposed estimator was proved to be design-unbiased and asymptotically design-consistent. a asymptotic normality was proved under both design and combined spaces. two data driven estimators are proposed based on a proposed oracle estimator. we show a efficiency of a proposed estimators inside simulation studies. the real survey data example was provided considering an illustration.",0,1,0,1
13439,"the dirichlet $k$-partition of the domain $u \subseteq \mathbb{r}^d$ was the collection of $k$ pairwise disjoint open subsets such that a sum of their first laplace-dirichlet eigenvalues was minimal. the discrete version of dirichlet partitions has been posed on graphs with applications inside data analysis. both versions admit variational formulations: solutions are characterized by minimizers of a dirichlet energy of mappings from $u$ into the singular space $\sigma_k \subseteq \mathbb{r}^k$. inside this paper, we extend results of n.\ garc?a trillos and d.\ slep?ev to show that there exist solutions of a continuum problem arising as limits to solutions of the sequence of discrete problems. specifically, the sequence of points $\{x_i\}_{i \in \mathbb{n}}$ from $u$ was sampled i.i.d.\ with respect to the given probability measure $\nu$ on $u$ and considering all $n \in \mathbb{n}$, the geometric graph $g_n$ was constructed from a first $n$ points $x_1, x_2, \ldots, x_n$ and a pairwise distances between a points. with probability one with respect to a choice of points $\{x_i\}_{i \in \mathbb{n}}$, we show that as $n \to \infty$ a discrete dirichlet energies considering functions $g_n \to \sigma_k$ $\gamma$-converge to (a scalar multiple of) a continuum dirichlet energy considering functions $u \to \sigma_k$ with respect to the metric coming from a theory of optimal transport. this, along with the compactness property considering a aforementioned energies that we prove, implies a convergence of minimizers. when $\nu$ was a uniform distribution, our results also imply a statistical consistency statement that dirichlet partitions of geometric graphs converge to partitions of a sampled space inside a hausdorff sense.",0,1,0,1
17709,"a processes that led to a formation of a planetary bodies inside a solar system are still not fully understood. with the help of a results obtained with a comprehensive suite of instruments on-board esa's rosetta mission, we present evidence that comet 67p/churyumov-gerasimenko likely formed through a gentle gravitational collapse of the bound clump of mm-sized dust aggregates (""pebbles""), intermixed with microscopic ice particles. this formation scenario leads to the cometary make-up that was simultaneously compatible with a global porosity, homogeneity, tensile strength, thermal inertia, vertical temperature profiles, sizes and porosities of emitted dust, and a steep increase inside water-vapour production rate with decreasing heliocentric distance, measured by a instruments on-board a rosetta spacecraft and a philae lander. our findings suggest that a pebbles observed to be abundant inside protoplanetary discs around young stars provide a building material considering comets and other minor bodies.",0,0,1,0
19259,"inside this paper, we have discussed the quantum idea behind the method considering a all-pair multiclass classification problem. we have shown that a multiclass support vector machine considering big data classification with the quantum all-pair idea behind the method should be implemented inside logarithm runtime complexity on the quantum computer. inside an all-pair approach, there was one binary classification problem considering each pair of classes, and so there are k (k-1)/2 classifiers considering the k-class problem. as compared to a classical multiclass support vector machine that should be implemented with polynomial run time complexity, our idea behind the method exhibits exponential speed up inside a quantum version. a quantum all-pair algorithm should be used with other classification algorithms, and the speed up gain should be achieved as compared to their classical counterparts.",1,0,0,0
15824,"we classify all cubic extensions of any field of arbitrary characteristic, up to isomorphism, using an explicit construction involving three fundamental types of cubic forms. we deduce the classification of any galois cubic extension of the field. a splitting and ramification of places inside the separable cubic extension of any global function field are completely determined, and precise riemann-hurwitz formulae are given. inside doing so, we determine a decomposition of any cubic polynomial over the finite field.",0,1,0,0
12003,we present the family of easily computable upper bounds considering a holevo quantity of ensemble of quantum states depending on the reference state as the free parameter. these upper bounds are obtained by combining probabilistic and metric characteristics of a ensemble. we show that appropriate choice of a reference state gives tight upper bounds considering a holevo quantity which inside many cases improve existing estimates inside a literature. we also present upper bound considering a holevo quantity of the generalized ensemble of quantum states with finite average energy depending on metric divergence of a ensemble. a specification of this upper bound considering a multi-mode quantum oscillator was tight considering large energy. a above results are used to obtain tight upper bounds considering a holevo capacity of finite-dimensional and infinite-dimensional energy-constrained quantum channels depending on metric characteristics of a channel output.,1,0,0,0
12691,"most previous work of centralities focuses on metrics of vertex importance and methods considering identifying powerful vertices, while related work considering edges was much lesser, especially considering weighted networks, due to a computational challenge. inside this paper, we propose to use a well-known kirchhoff index as a measure of edge centrality inside weighted networks, called $\theta$-kirchhoff edge centrality. a kirchhoff index of the network was defined as a sum of effective resistances over all vertex pairs. a centrality of an edge $e$ was reflected inside a increase of kirchhoff index of a network when a edge $e$ was partially deactivated, characterized by the parameter $\theta$. we define two equivalent measures considering $\theta$-kirchhoff edge centrality. both are global metrics and have the better discriminating power than commonly used measures, based on local or partial structural information of networks, e.g. edge betweenness and spanning edge centrality. despite a strong advantages of kirchhoff index as the centrality measure and its wide applications, computing a exact value of kirchhoff edge centrality considering each edge inside the graph was computationally demanding. to solve this problem, considering each of a $\theta$-kirchhoff edge centrality metrics, we present an efficient algorithm to compute its $\epsilon$-approximation considering all a $m$ edges inside nearly linear time inside $m$. a proposed $\theta$-kirchhoff edge centrality was a first global metric of edge importance that should be provably approximated inside nearly-linear time. moreover, according to a $\theta$-kirchhoff edge centrality, we present the $\theta$-kirchhoff vertex centrality measure, as well as the fast algorithm that should compute $\epsilon$-approximate kirchhoff vertex centrality considering all a $n$ vertices inside nearly linear time inside $m$.",1,0,0,0
9320,"a lack of interpretability often makes black-box models difficult to be applied to many practical domains. considering this reason, a current work, from a black-box model input port, proposes to incorporate data-based prior information into a black-box soft-margin svm model to enhance its interpretability. a concept and incorporation mechanism of data-based prior information are successively developed, based on which a interpretable or partly interpretable svm optimization model was designed and then solved through handily rewriting a optimization problem as the nonlinear quadratic programming problem. an algorithm considering mining data-based linear prior information from data set was also proposed, which generates the linear expression with respect to two appropriate inputs identified from all inputs of system. at last, a proposed interpretability enhancement strategy was applied to eight benchmark examples considering effectiveness exhibition.",1,0,0,1
4843,"diffusion and flow-driven instability, or transport-driven instability, was one of a central mechanisms to generate inhomogeneous gradient of concentrations inside spatially distributed chemical systems. however, verifying a transport-driven instability of reaction-diffusion-advection systems requires checking a jacobian eigenvalues of infinitely many fourier modes, which was computationally intractable. to overcome this limitation, this paper proposes mathematical optimization algorithms that determine a stability/instability of reaction-diffusion-advection systems by finite steps of algebraic calculations. specifically, a stability/instability analysis of fourier modes was formulated as the sum-of-squares (sos) optimization program, which was the class of convex optimization whose solvers are widely available as software packages. a optimization program was further extended considering facile computation of a destabilizing spatial modes. this extension allows considering predicting and designing a shape of concentration gradient without simulating a governing equations. a streamlined analysis process of self-organized pattern formation was demonstrated with the simple illustrative reaction model with diffusion and advection.",1,0,0,0
13972,"graphs are an important tool to model data inside different domains, including social networks, bioinformatics and a world wide web. most of a networks formed inside these domains are directed graphs, where all a edges have the direction and they are not symmetric. betweenness centrality was an important index widely used to analyze networks. inside this paper, first given the directed network $g$ and the vertex $r \in v(g)$, we propose the new exact algorithm to compute betweenness score of $r$. our algorithm pre-computes the set $\mathcal{rv}(r)$, which was used to prune the huge amount of computations that do not contribute inside a betweenness score of $r$. time complexity of our exact algorithm depends on $|\mathcal{rv}(r)|$ and it was respectively $\theta(|\mathcal{rv}(r)|\cdot|e(g)|)$ and $\theta(|\mathcal{rv}(r)|\cdot|e(g)|+|\mathcal{rv}(r)|\cdot|v(g)|\log |v(g)|)$ considering unweighted graphs and weighted graphs with positive weights. $|\mathcal{rv}(r)|$ was bounded from above by $|v(g)|-1$ and inside most cases, it was the small constant. then, considering a cases where $\mathcal{rv}(r)$ was large, we present the simple randomized algorithm that samples from $\mathcal{rv}(r)$ and performs computations considering only a sampled elements. we show that this algorithm provides an $(\epsilon,\delta)$-approximation of a betweenness score of $r$. finally, we perform extensive experiments over several real-world datasets from different domains considering several randomly chosen vertices as well as considering a vertices with a highest betweenness scores. our experiments reveal that inside most cases, our algorithm significantly outperforms a most efficient existing randomized algorithms, inside terms of both running time and accuracy. our experiments also show that our proposed algorithm computes betweenness scores of all vertices inside a sets of sizes 5, 10 and 15, much faster and more accurate than a most efficient existing algorithms.",1,0,0,0
4857,"changing institution was the scientist's key career decision, which plays an important role inside education, scientific productivity, and a generation of scientific knowledge. yet, our understanding of a factors influencing the relocation decision was very limited. inside this paper we investigate how a scientific profile of the scientist determines their decision to move (i.e., change institution). to this aim, we describe the scientist's profile by three main aspects: a scientist's recent scientific career, a quality of their scientific environment and a structure of their scientific collaboration network. we then design and implement the two-stage predictive model: first, we use data mining to predict which researcher will move inside a next year on a basis of their scientific profile; second we predict which institution they will choose by with the help of the novel social-gravity model, an adaptation of a traditional gravity model of human mobility. experiments on the massive dataset of scientific publications show that our idea behind the method performs well inside both a stages, resulting inside the 85% reduction of a prediction error with respect to a state-of-the-art approaches.",1,0,0,1
16341,"we consider the stochastic nonlinear schr?dinger equation with multiplicative noise inside an abstract framework that covers subcritical focusing and defocusing stochastic nls inside $h^1$ on compact manifolds and bounded domains. we construct the martingale solution with the help of the modified faedo-galerkin-method based on a littlewood-paley-decomposition. considering 2d manifolds with bounded geometry, we use strichartz estimates to show pathwise uniqueness.",0,1,0,0
17837,"we explore the new mechanism to explain polarization phenomena inside opinion dynamics inside which agents evaluate alternative views on a basis of a social feedback obtained on expressing them. high support of a favored opinion inside a social environment, was treated as the positive feedback which reinforces a value associated to this opinion. inside connected networks of sufficiently high modularity, different groups of agents should form strong convictions of competing opinions. linking a social feedback process to standard equilibrium concepts we analytically characterize sufficient conditions considering a stability of bi-polarization. while previous models have emphasized a polarization effects of deliberative argument-based communication, our model highlights an affective experience-based route to polarization, without assumptions about negative influence or bounded confidence.",1,0,0,0
19225,"with the help of the mix of numerical and analytic methods, we show that recent nmr $^{17}$o measurements provide detailed information about a structure of a charge-density wave (cdw) phase inside underdoped yba$_2$cu$_3$o$_{6+x}$. we perform bogoliubov-de gennes (bdg) calculations of both a local density of states and a orbitally resolved charge density, which are closely related to a magnetic and electric quadrupole contributions to a nmr spectrum, with the help of the microscopic model that is shown previously to agree closely with x-ray experiments. a bdg results reproduce qualitative features of a experimental spectrum extremely well. these results are interpreted inside terms of the generic ""hotspot"" model that allows one to trace a origins of a nmr lineshapes. we find that four quantities---the orbital character of a fermi surface at a hotspots, a fermi surface curvature at a hotspots, a cdw correlation length, and a magnitude of a subdominant cdw component---are key inside determining a lineshapes.",0,0,1,0
5421,"the nonuniform neumann boundary-value problem was considered considering a poisson equation inside the thin $3d$ aneurysm-type domain that consists of thin curvilinear cylinders that are joined through an aneurysm of diameter $\mathcal{o}(\varepsilon).$ the rigorous procedure was developed to construct a complete asymptotic expansion considering a solution as a parameter $\varepsilon \to 0.$ a asymptotic expansion consists of the regular part that was located in of each cylinder, the boundary-layer part near a base of each cylinder, and an inner part discovered inside the neighborhood of a aneurysm. terms of a inner part of a asymptotics are special solutions of boundary-value problems inside an unbounded domain with different outlets at infinity. it turns out that they have polynomial growth at infinity. by matching these parts, we derive a limit problem $(\varepsilon =0)$ inside a corresponding graph and the recurrence procedure to determine all terms of a asymptotic expansion. energetic and uniform pointwise estimates are proved. these estimates allow us to observe a impact of a aneurysm.",0,1,0,0
6531,"dimension reduction and visualization was the staple of data analytics. methods such as principal component analysis (pca) and multidimensional scaling (mds) provide low dimensional (ld) projections of high dimensional (hd) data while preserving an hd relationship between observations. traditional biplots assign meaning to a ld space of the pca projection by displaying ld axes considering a attributes. these axes, however, are specific to a linear projection used inside pca. mds projections, which allow considering arbitrary stress and dissimilarity functions, require special care when labeling a ld space. we propose an iterative scheme to plot an ld axis considering each attribute based on a user-specified stress and dissimilarity metrics. we discuss a details of our general biplot methodology, its relationship with pca-derived biplots, and provide examples with the help of real data.",0,0,0,1
307,"we consider a problem of decentralized consensus optimization, where a sum of $n$ smooth and strongly convex functions are minimized over $n$ distributed agents that form the connected network. inside particular, we consider a case that a communicated local decision variables among nodes are quantized inside order to alleviate a communication bottleneck inside distributed optimization. we propose a quantized decentralized gradient descent (qdgd) algorithm, inside which nodes update their local decision variables by combining a quantized information received from their neighbors with their local information. we prove that under standard strong convexity and smoothness assumptions considering a objective function, qdgd achieves the vanishing mean solution error under customary conditions considering quantizers. to a best of our knowledge, this was a first algorithm that achieves vanishing consensus error inside a presence of quantization noise. moreover, we provide simulation results that show tight agreement between our derived theoretical convergence rate and a numerical results.",0,0,0,1
3381,a eigenstructure of a discrete fourier transform (dft) was examined and new systematic procedures to generate eigenvectors of a unitary dft are proposed. dft eigenvectors are suggested as user signatures considering data communication over a real adder channel (rac). a proposed multiuser communication system over a 2-user rac was detailed.,1,0,0,1
14050,"we study a skip-thought model with neighborhood information as weak supervision. more specifically, we propose the skip-thought neighbor model to consider a adjacent sentences as the neighborhood. we train our skip-thought neighbor model on the large corpus with continuous sentences, and then evaluate a trained model on 7 tasks, which include semantic relatedness, paraphrase detection, and classification benchmarks. both quantitative comparison and qualitative investigation are conducted. we empirically show that, our skip-thought neighbor model performs as well as a skip-thought model on evaluation tasks. inside addition, we found that, incorporating an autoencoder path inside our model didn't aid our model to perform better, while it hurts a performance of a skip-thought model.",1,0,0,0
19693,several conjectural continued fractions found with a aid of various algorithms are published inside this paper.,0,1,0,0
5568,"transferring solutions found by trajectory optimization to robotic hardware remains the challenging task. when a optimization fully exploits a provided model to perform dynamic tasks, a presence of unmodeled dynamics renders a motion infeasible on a real system. model errors should be the result of model simplifications, but also naturally arise when deploying a robot inside unstructured and nondeterministic environments. predominantly, compliant contacts and actuator dynamics lead to bandwidth limitations. bandwidth limits arising from compliant contacts and actuator dynamics tend to occur at high frequencies. while classical control methods provide tools to synthesize controllers that are robust to the class of model errors, such the notion was missing inside modern trajectory optimization, which was solved inside a time domain. we propose frequency-shaped cost functions to achieve robust solutions inside a context of optimal control considering legged robots. through simulation and hardware experiments we show that motion plans should be made compatible with bandwidth limits set by actuators and contact dynamics. a smoothness of a model predictive solutions should be continuously tuned without compromising a feasibility of a problem. experiments with a quadrupedal robot anymal, which was driven by high-compliant series elastic actuators, showed significantly better tracking performance of a planned motion, torque, and force trajectories and enabled a machine to walk robustly on ground with unmodeled compliance.",1,0,0,0
17222,"we apply a tractor image modeling code to improve upon existing multi-band photometry considering a spitzer extragalactic representative volume survey (servs). servs consists of post-cryogenic spitzer observations at 3.6 and 4.5 micron over five well-studied deep fields spanning 18 square degrees. inside concert with data from ground-based near-infrared (nir) and optical surveys, servs aims to provide the census of a properties of massive galaxies out to z ~ 5. to accomplish this, we are with the help of a tractor to perform ""forced photometry."" this technique employs prior measurements of source positions and surface brightness profiles from the high-resolution fiducial band from a vista deep extragalactic observations (video) survey to model and fit a fluxes at lower-resolution bands. we discuss our implementation of a tractor over the square degree test region within a xmm-lss field with deep imaging inside 12 nir/optical bands. our new multi-band source catalogs offer the number of advantages over traditional position-matched catalogs, including 1) consistent source cross-identification between bands, 2) de-blending of sources that are clearly resolved inside a fiducial band but blended inside a lower-resolution servs data, 3) the higher source detection fraction inside each band, 4) the larger number of candidate galaxies inside a redshift range 5 < z < 6, and 5) the statistically significant improvement inside a photometric redshift accuracy as evidenced by a significant decrease inside a fraction of outliers compared to spectroscopic redshifts. thus, forced photometry with the help of a tractor offers the means of improving a accuracy of multi-band extragalactic surveys designed considering galaxy evolution studies. we will extend our application of this technique to a full servs footprint inside a future.",0,0,1,0
19141,"gravitation, a universal attractive force, acts upon all matter (and radiation) relentlessly. left to itself, gravity would pull everything together and a universe would be nothing but the gigantic black hole. nature throws almost every bit of physics - rotation, magnetic field, heat, quantum effects and so on, at gravity to escape such the fate. inside this series of articles we shall explore systems where a eternal pull of gravity has been held off by one or another such means.",0,0,1,0
10891,"we demonstrate the thermodynamic formulation to quantify defect formation energetics inside an insulator under high electric field. as the model system, we analyzed neutral oxygen vacancies (color centers) inside alkaline-earth-metal binary oxides with the help of density functional theory, berry phase calculations, and maximally localized wannier functions. work of polarization lowers a field dependent electric gibbs energy of formation of this defect. this was attributed mainly to a ease of polarizing a two electrons trapped inside a vacant site, and secondarily to a defect induced reduction inside bond stiffness and softening of phonon modes. a formulation and analysis have implications considering understanding a behavior of insulating oxides inside electronic, magnetic, catalytic, and electrocaloric devices under high electric field.",0,0,1,0
15756,"a effects of mhd boundary layer flow of non-linear thermal radiation with convective heat transfer and non-uniform heat source/sink inside presence of thermophortic velocity and chemical reaction investigated inside this study. suitable similarity transformation are used to solve a partial ordinary differential equation of considered governing flow. runge-kutta fourth fifth order fehlberg method with shooting techniques are used to solved non-dimensional governing equations. a variation of different parameters such as thermophoretic parameter, chemical reaction parameter, non- uniform heat source/sink parameters are studied on velocity, temperature and concentration profiles, and are described by suitable graphs and tables. a obtained results are inside very well agreement with previous results.",0,0,1,0
351,"a sparsity and compressibility of finite-dimensional signals are of great interest inside fields such as compressed sensing. a notion of compressibility was also extended to infinite sequences of i.i.d. or ergodic random variables based on a observed error inside their nonlinear k-term approximation. inside this work, we use a entropy measure to study a compressibility of continuous-domain innovation processes (alternatively known as white noise). specifically, we define such the measure as a entropy limit of a doubly quantized (time and amplitude) process. this provides the tool to compare a compressibility of various innovation processes. it also allows us to identify an analogue of a concept of ""entropy dimension"" which is originally defined by r?nyi considering random variables. particular attention was given to stable and impulsive poisson innovation processes. here, our results recognize poisson innovations as a more compressible ones with an entropy measure far below that of stable innovations. while this result departs from a previous knowledge regarding a compressibility of fat-tailed distributions, our entropy measure ranks stable innovations according to their tail decay.",1,0,0,0
8919,model predictive control (mpc) method was the class of advanced control techniques most widely applied inside industry. a major advantages of a mpc are its straightforward procedure which should be applied considering both linear and nonlinear system. this paper proposes a use of mpc considering voltage source converter (vsc) inside the high voltage direct current (hvdc) system. the mpc controller was modeled based on a state-space model of the single vsc-hvdc station including a dynamics of a main ac grid. the full scale nonlinear switching model of point-to-point connected vsc-based hvdc system was developed inside matlab/simulink association with simpower system to demonstrate a application of a proposed controller.,1,0,0,0
6329,"automatic continuous time, continuous value assessment of the patient's pain from face video was highly sought after by a medical profession. despite a recent advances inside deep learning that attain impressive results inside many domains, pain approximation risks not being able to benefit from this due to a difficulty inside obtaining data sets of considerable size. inside this work we propose the combination of hand-crafted and deep-learned features that makes a most of deep learning techniques inside small sample settings. encoding shape, appearance, and dynamics, our method significantly outperforms a current state of a art, attaining the rmse error of less than 1 point on the 16-level pain scale, whilst simultaneously scoring the 67.3% pearson correlation coefficient between our predicted pain level time series and a ground truth.",1,0,0,0
9917,"we study a behavior of the fundamental tool inside sparse statistical modeling --the best-subset selection procedure (aka ""best-subsets""). assuming that a underlying linear model was sparse, it was well known, both inside theory and inside practice, that a best-subsets procedure works extremely well inside terms of several statistical metrics (prediction, approximation and variable selection) when a signal to noise ratio (snr) was high. however, its performance degrades substantially when a snr was low -- it was outperformed inside predictive accuracy by continuous shrinkage methods, such as ridge regression and a lasso. we explain why this behavior should not come as the surprise, and contend that a original version of a classical best-subsets procedure was, perhaps, not designed to be used inside a low snr regimes. we propose the close cousin of best-subsets, namely, its $\ell_{q}$-regularized version, considering $q \in\{1, 2\}$, which (a) mitigates, to the large extent, a poor predictive performance of best-subsets inside a low snr regimes; (b) performs favorably and generally delivers the substantially sparser model when compared to a best predictive models available using ridge regression and a lasso. our estimator should be expressed as the solution to the mixed integer second order conic optimization problem and, hence, was amenable to modern computational tools from mathematical optimization. we explore a theoretical properties of a predictive capabilities of a proposed estimator and complement our findings using several numerical experiments.",0,1,0,1
1191,"we consider the restless multi-armed bandit (rmab) inside which there are two types of arms, say the and b. each arm should be inside one of two states, say $0$ or $1.$ playing the type the arm brings it to state $0$ with probability one and not playing it induces state transitions with arm-dependent probabilities. whereas playing the type b arm leads it to state $1$ with probability $1$ and not playing it gets state that dependent on transition probabilities of arm. further, play of an arm generates the unit reward with the probability that depends on a state of a arm. a belief about a state of a arm should be calculated with the help of the bayesian update after every play. this rmab has been designed considering use inside recommendation systems where a user's preferences depend on a history of recommendations. this rmab should also be used inside applications like creating of playlists or placement of advertisements. inside this paper we formulate a long term reward maximization problem as infinite horizon discounted reward and average reward problem. we analyse a rmab by first studying discounted reward scenario. we show that it was whittle-indexable and then obtain the closed form expression considering a whittle index considering each arm calculated from a belief about its state and a parameters that describe a arm. we next analyse a average reward problem with the help of vanishing discounted idea behind the method and derive a closed form expression considering whittle index. considering the rmab to be useful inside practice, we need to be able to learn a parameters of a arms. we present an algorithm derived from thompson sampling scheme, that learns a parameters of a arms and also illustrate its performance numerically.",1,0,0,0
2020,"a popular adjusted rand index (ari) was extended to a task of simultaneous clustering of a rows and columns of the given matrix. this new index called coclustering adjusted rand index (cari) remains convenient and competitive facing other indices. indeed, partitions with high number of clusters should be considered and it does not require any convention when a numbers of clusters inside partitions are different. experiments on simulated partitions are presented and a performance of this index to measure a agreement between two pairs of partitions was assessed. comparison with other indices was discussed.",0,0,0,1
19870,"today, smartphone devices are owned by the large portion of a population and have become the very popular platform considering accessing a internet. smartphones provide a user with immediate access to information and services. however, they should easily expose a user to many privacy risks. applications that are installed on a device and entities with access to a device's internet traffic should reveal private information about a smartphone user and steal sensitive content stored on a device or transmitted by a device over a internet. inside this paper, we present the method to reveal various demographics and technical computer skills of smartphone users by their internet traffic records, with the help of machine learning classification models. we implement and evaluate a method on real life data of smartphone users and show that smartphone users should be classified by their gender, smoking habits, software programming experience, and other characteristics.",1,0,0,0
17154,"we have developed an efficient active galactic nucleus (agn) selection method with the help of 18-band spectral energy distribution (sed) fitting inside mid-infrared (mid-ir). agns are often obscured by gas and dust, and those obscured agns tend to be missed inside optical, uv and soft x-ray observations. mid-ir light should aid us to recover them inside an obscuration free way with the help of their thermal emission. on a other hand, star-forming galaxies (sfg) also have strong pah emission features inside mid-ir. hence, establishing an accurate method to separate populations of agn and sfg was important. however, inside previous mid-ir surveys, only 3 or 4 filters were available, and thus a selection is limited. we combined akari's continuous 9 mid-ir bands with wise and spitzer data to create 18 mid-ir bands considering agn selection. among 4682 galaxies inside a akari nep deep field, 1388 are selected to be agn hosts, which implies an agn fraction of 29.6$\pm$0.8$\%$ (among them 47$\%$ are seyfert 1.8 and 2). comparing a result from sed fitting into wise and spitzer colour-colour diagram reveals that seyferts are often missed by previous studies. our result has been tested by stacking median magnitude considering each sample. with the help of x-ray data from chandra, we compared a result of our sed fitting with wise's colour box selection. we recovered more x-ray detected agn than previous methods by 20$\%$.",0,0,1,0
666,"we report constraints on a global $21$ cm signal due to neutral hydrogen at redshifts $14.8 \geq z \geq 6.5$. we derive our constraints from low foreground observations of a average sky brightness spectrum conducted with a edges high-band instrument between september $7$ and october $26$, $2015$. observations were calibrated by accounting considering a effects of antenna beam chromaticity, antenna and ground losses, signal reflections, and receiver parameters. we evaluate a consistency between a spectrum and phenomenological models considering a global $21$ cm signal. considering tanh-based representations of a ionization history during a epoch of reionization, we rule out, at $\geq2\sigma$ significance, models with duration of up to $\delta z = 1$ at $z\approx8.5$ and higher than $\delta z = 0.4$ across most of a observed redshift range under a usual assumption that a $21$ cm spin temperature was much larger than a temperature of a cosmic microwave background (cmb) during reionization. we also investigate the `cold' igm scenario that assumes perfect ly$\alpha$ coupling of a $21$ cm spin temperature to a temperature of a intergalactic medium (igm), but that a igm was not heated by early stars or stellar remants. under this assumption, we reject tanh-based reionization models of duration $\delta z \lesssim 2$ over most of a observed redshift range. finally, we explore and reject the broad range of gaussian models considering a $21$ cm absorption feature expected inside a first light era. as an example, we reject $100$ mk gaussians with duration (full width at half maximum) $\delta z \leq 4$ over a range $14.2\geq z\geq 6.5$ at $\geq2\sigma$ significance.",0,0,1,0
11729,"we discuss a relative merits of optimistic and randomized approaches to exploration inside reinforcement learning. optimistic approaches presented inside a literature apply an optimistic boost to a value approximate at each state-action pair and select actions that are greedy with respect to a resulting optimistic value function. randomized approaches sample from among statistically plausible value functions and select actions that are greedy with respect to a random sample. prior computational experience suggests that randomized approaches should lead to far more statistically efficient learning. we present two simple analytic examples that elucidate why this was a case. inside principle, there should be optimistic approaches that fare well relative to randomized approaches, but that would require intractable computation. optimistic approaches that have been proposed inside a literature sacrifice statistical efficiency considering a sake of computational efficiency. randomized approaches, on a other hand, may enable simultaneous statistical and computational efficiency.",1,0,0,1
19654,"recent successes inside reinforcement learning have lead to a development of complex controllers considering real-world robots. as these robots are deployed inside safety-critical applications and interact with humans, it becomes critical to ensure safety inside order to avoid causing harm. the first step inside this direction was to test a controllers inside simulation. to be able to do this, we need to capture what we mean by safety and then efficiently search a space of all behaviors to see if they are safe. inside this paper, we present an active-testing framework based on bayesian optimization. we specify safety constraints with the help of logic and exploit structure inside a problem inside order to test a system considering adversarial counter examples that violate a safety specifications. these specifications are defined as complex boolean combinations of smooth functions on a trajectories and, unlike reward functions inside reinforcement learning, are expressive and impose hard constraints on a system. inside our framework, we exploit regularity assumptions on individual functions inside form of the gaussian process (gp) prior. we combine these into the coherent optimization framework with the help of problem structure. a resulting algorithm was able to provably verify complex safety specifications or alternatively find counter examples. experimental results show that a proposed method was able to find adversarial examples quickly.",1,0,0,1
10119,"an evaluation metric was an absolute necessity considering measuring a performance of any system and complexity of any data. inside this paper, we have discussed how to determine a level of complexity of code-mixed social media texts that are growing rapidly due to multilingual interference. inside general, texts written inside multiple languages are often hard to comprehend and analyze. at a same time, inside order to meet a demands of analysis, it was also necessary to determine a complexity of the particular document or the text segment. thus, inside a present paper, we have discussed a existing metrics considering determining a code-mixing complexity of the corpus, their advantages, and shortcomings as well as proposed several improvements on a existing metrics. a new index better reflects a variety and complexity of the multilingual document. also, a index should be applied to the sentence and seamlessly extended to the paragraph or an entire document. we have employed two existing code-mixed corpora to suit a requirements of our study.",1,0,0,0
18185,"we present an overview of scenarios where a observed dark matter (dm) abundance consists of feebly interacting massive particles (fimps), produced non-thermally by a so-called freeze-in mechanism. inside contrast to a usual freeze-out scenario, frozen-in fimp dm interacts very weakly with a particles inside a visible sector and never attained thermal equilibrium with a baryon-photon fluid inside a early universe. instead of being determined by its annihilation strength, a dm abundance depends on a decay and annihilation strengths of particles inside equilibrium with a baryon-photon fluid, as well as couplings inside a dm sector. this makes frozen-in dm very difficult but not impossible to test. inside this review, we present a freeze-in mechanism and its variations considered inside a literature (dark freeze-out and reannihilation), compare them to a standard dm freeze-out scenario, discuss several aspects of model building, and pay particular attention to observational properties and general testability of such feebly interacting dm.",0,0,1,0
